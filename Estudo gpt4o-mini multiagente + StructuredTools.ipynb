{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.3.25)\n",
      "Requirement already satisfied: openai in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.84.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.3.0)\n",
      "Requirement already satisfied: pandasql in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.7.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain_openai in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.3.65)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (2.11.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (3.12.7)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain_openai) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\Pichau\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# 0) Instalar todas as dependências necessárias\n",
    "%pip install langchain langchain-community openai pandas pandasql python-dotenv langchain_openai langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Importação das bibliotecas necessárias\n",
    "from langchain_openai import ChatOpenAI # Alterado para langchain_openai\n",
    "from langchain.agents import initialize_agent, AgentType # Usado para os sub-agentes\n",
    "from langchain_core.tools import StructuredTool # Para ferramentas com schema\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory # Importação da memória / definição de contexto para armazenamento da memória\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder # Para o agente principal\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent # Para o agente principal\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from pandasql import sqldf\n",
    "from io import TextIOWrapper\n",
    "import re\n",
    "from pydantic import BaseModel, Field # Necessário para StructuredTool\n",
    "from langchain_core.messages import HumanMessage, AIMessage \n",
    "\n",
    "# Variável global para armazenar o dataframe\n",
    "df_global = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Criação da função das ferramentas\n",
    "def carregar_arquivos(arquivo_zip_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Quando necessário extrai os arquivos do formato .zip (Extrair,Extract,Descompactar,Unzip,Abrir,Open,Deszipar,Decompress,Desempacotar,Unarchive,Desarquivar)\n",
    "    Após carregar e mesclar, informa o número de linhas, colunas e a lista das colunas disponíveis.\n",
    "    Exemplo: 'C:\\\\Users\\Pichau\\Downloads\\\\teste\\\\202401_NFs.zip'\n",
    "    \"\"\"\n",
    "    arquivo_zip_path = arquivo_zip_path.strip(\"'\\\"\")\n",
    "    global df_global # Declara que você vai modificar a variável global\n",
    "    grupo_csv = {}\n",
    "    \n",
    "    try:\n",
    "        with zipfile.ZipFile(arquivo_zip_path, 'r') as zip_ref:\n",
    "            csv_files_in_zip = [f for f in zip_ref.namelist() if f.endswith('.csv')]\n",
    "            \n",
    "            if len(csv_files_in_zip) < 2:\n",
    "                return f\"Erro: O arquivo ZIP '{os.path.basename(arquivo_zip_path)}' precisa conter pelo menos dois arquivos CSV para serem mesclados. Foram encontrados {len(csv_files_in_zip)} CSVs.\"\n",
    "            \n",
    "            for arquivo_csv in csv_files_in_zip:\n",
    "                with zip_ref.open(arquivo_csv) as arq:\n",
    "                    df = pd.read_csv(TextIOWrapper(arq,'utf-8', errors='ignore'))\n",
    "                    grupo_csv[arquivo_csv] = df\n",
    "            \n",
    "        sorted_keys = sorted(grupo_csv.keys())\n",
    "        primeiro_df = grupo_csv[sorted_keys[0]]\n",
    "        segundo_df = grupo_csv[sorted_keys[1]]\n",
    "        \n",
    "        # Tentativa de merge mais robusta: verificar se 'CHAVE DE ACESSO' existe em ambos\n",
    "        if 'CHAVE DE ACESSO' not in primeiro_df.columns or 'CHAVE DE ACESSO' not in segundo_df.columns:\n",
    "            return f\"Erro: A coluna 'CHAVE DE ACESSO' não foi encontrada em um ou ambos os arquivos CSV ({sorted_keys[0]}, {sorted_keys[1]}). Não foi possível mesclar.\"\n",
    "\n",
    "        df_global = pd.merge(primeiro_df, segundo_df, on='CHAVE DE ACESSO', how='left')\n",
    "        df_global.columns = df_global.columns.str.replace('_x$', '', regex=True)\n",
    "        \n",
    "        # Remover colunas _y de forma mais segura\n",
    "        cols_to_drop_y = [col for col in df_global.columns if col.endswith('_y')]\n",
    "        # Lista de colunas fixas a serem removidas (se existirem e terminarem com _y)\n",
    "        fixed_y_cols = ['MODELO_y','SÉRIE_y','NÚMERO_y','NATUREZA DA OPERAÇÃO_y','DATA EMISSÃO_y','CPF/CNPJ Emitente_y','RAZÃO SOCIAL EMITENTE_y','INSCRIÇÃO ESTADUAL EMITENTE_y','UF EMITENTE_y','MUNICÍPIO EMITENTE_y','CNPJ DESTINATÁRIO_y','NOME DESTINATÁRIO_y','UF DESTINATÁRIO_y','INDICADOR IE DESTINATÁRIO_y','DESTINO DA OPERAÇÃO_y','CONSUMIDOR FINAL_y','PRESENÇA DO COMPRADOR_y']\n",
    "        \n",
    "        # Filtra as colunas que realmente existem no DataFrame antes de dropar\n",
    "        cols_to_drop_final = [col for col in (cols_to_drop_y + fixed_y_cols) if col in df_global.columns]\n",
    "        df_global = df_global.drop(columns=list(set(cols_to_drop_final))) # Usar set para evitar duplicatas e converter para lista\n",
    "\n",
    "        colunas_originais = df_global.columns.tolist()\n",
    "    \n",
    "        mapeamento = {}\n",
    "        for col in colunas_originais:\n",
    "            col_nova = col.upper()\n",
    "            col_nova = re.sub(r\"[^A-Z0-9]\", \"_\", col_nova)  # Substitui espaços/símbolos por _\n",
    "            col_nova = re.sub(r\"_+\", \"_\", col_nova).strip(\"_\")  # Remove múltiplos _\n",
    "            mapeamento[col] = col_nova\n",
    "\n",
    "        df_global.columns = [mapeamento[col] for col in df_global.columns]\n",
    "       \n",
    "        return f\"Arquivos carregados e mesclados com sucesso! DataFrame pronto para análise com {len(df_global)} linhas e {len(df_global.columns)} colunas. As colunas disponíveis são: {list(df_global.columns)}.\"\n",
    "    except FileNotFoundError:\n",
    "        return f\"Erro: O arquivo ZIP '{arquivo_zip_path}' não foi encontrado. Verifique o caminho.\"\n",
    "    except zipfile.BadZipFile:\n",
    "        return f\"Erro: O arquivo '{arquivo_zip_path}' não é um arquivo ZIP válido ou está corrompido.\"\n",
    "    except Exception as e:\n",
    "        return f\"Erro inesperado ao carregar ou processar o arquivo ZIP: {e}. Detalhes: {str(e)}\"\n",
    "\n",
    "def consulta_sql(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Executa uma consulta SQL no DataFrame df_global.\n",
    "    Exemplo: SELECT SUM(\"VALOR TOTAL\") FROM df_global WHERE \"CPF/CNPJ DESTINATÁRIO\" = '12.345.678/0001-90'\n",
    "    \"\"\"\n",
    "    global df_global\n",
    "    if df_global is None:\n",
    "        return \"O DataFrame global ainda não foi carregado. Por favor, use a ferramenta 'carregar_arquivos' primeiro.\"\n",
    "\n",
    "    try:\n",
    "        # Prepara o ambiente para pandasql, garantindo que df_global esteja acessível\n",
    "        globals_dict = {'df_global': df_global}\n",
    "\n",
    "        result = sqldf(query, globals_dict)\n",
    "        if result.empty:\n",
    "            return \"A consulta SQL não retornou resultados.\"\n",
    "        return result.to_string(index=False)\n",
    "    except Exception as e:\n",
    "        # Mensagem de erro mais detalhada\n",
    "        cols_info = \"\"\n",
    "        if df_global is not None:\n",
    "            cols_info = f\"\\nColunas disponíveis: {list(df_global.columns)}.\"\n",
    "            # Tentar inferir se o erro é de tipo de dado ou coluna inexistente\n",
    "            if \"no such column\" in str(e).lower():\n",
    "                cols_info += \" Verifique se os nomes das colunas na sua consulta estão corretos e correspondem exatamente aos nomes listados.\"\n",
    "            elif \"no such table\" in str(e).lower() and 'df_global' not in query.lower():\n",
    "                cols_info += \" Lembre-se de referenciar o DataFrame como 'df_global' na sua consulta SQL.\"\n",
    "            elif \"could not convert string to float\" in str(e).lower() or \"numeric\" in str(e).lower():\n",
    "                 cols_info += \" Parece que você está tentando realizar uma operação numérica em uma coluna não numérica. Considere usar CAST() no SQL ou verificar o tipo da coluna com 'inspecionar_dataframe'.\"\n",
    "\n",
    "        return f\"Erro na consulta SQL: {e}.{cols_info}\"\n",
    "    \n",
    "\n",
    "def inspecionar_dataframe(args: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Retorna informações sobre o DataFrame global, incluindo as primeiras linhas,\n",
    "    nomes das colunas e seus respectivos tipos de dados (dtypes).\n",
    "    Útil para depurar consultas SQL, entender a estrutura dos dados ou verificar se o DataFrame foi carregado.\n",
    "    Não precisa de argumentos específicos, apenas chame a ferramenta para inspecionar.\n",
    "    \"\"\"\n",
    "    global df_global\n",
    "    if df_global is None:\n",
    "        return \"O DataFrame global ainda não foi carregado. Por favor, use a ferramenta 'carregar_arquivos' primeiro.\"\n",
    "    \n",
    "    info = f\"DataFrame carregado com {len(df_global)} linhas e {len(df_global.columns)} colunas.\\n\"\n",
    "    info += \"Colunas e Tipos de Dados:\\n\"\n",
    "    for col, dtype in df_global.dtypes.items():\n",
    "        info += f\"- '{col}': {dtype}\\n\"\n",
    "    info += \"\\nPrimeiras 5 linhas:\\n\"\n",
    "    # Adicionando um fallback caso o dataframe esteja vazio\n",
    "    if not df_global.empty:\n",
    "        info += df_global.head().to_string()\n",
    "    else:\n",
    "        info += \"O DataFrame está vazio.\"\n",
    "    return info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Escolhe o modelo selecionado e configuração de temperatura\n",
    "load_dotenv() # carrega o segredo\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    #model=\"openai/gpt-4o-mini\", # Ou \"gpt-3.5-turbo\"\n",
    "    #base_url=\"https://openrouter.ai/api/v1\",\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.15,\n",
    "    #api_key = os.getenv(\"open_key\"),\n",
    "    api_key = os.getenv(\"api_key\"),\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# --- Definição dos Schemas de Entrada para StructuredTool --- Essas classes, construídas com o Pydantic (BaseModel, Field), \n",
    "# são cruciais para a comunicação entre o LLM e suas ferramentas. \n",
    "# elas atuam como um \"contrato\" para cada ferramenta\n",
    "class CarregarArquivosInput(BaseModel):\n",
    "    arquivo_zip_path: str = Field(description=\"O caminho completo para o arquivo .zip a ser carregado.\")\n",
    "\n",
    "class ConsultaSQLInput(BaseModel):\n",
    "    query: str = Field(description=\"A consulta SQL a ser executada no DataFrame global (df_global).\")\n",
    "\n",
    "class InspecionarDataFrameInput(BaseModel):\n",
    "    args: str = Field(description=\"Argumento genérico, não é usado diretamente pela função, apenas para conformidade com StructuredTool.\")\n",
    "\n",
    "\n",
    "### Definição dos Sub-Agentes\n",
    "\n",
    "# Ferramentas específicas para cada sub-agente usando StructuredTool\n",
    "zip_tool = StructuredTool.from_function(\n",
    "    func=carregar_arquivos,\n",
    "    name=\"carregar_arquivos\",\n",
    "    description=carregar_arquivos.__doc__.strip(),\n",
    "    args_schema=CarregarArquivosInput\n",
    ")\n",
    "\n",
    "query_tool = StructuredTool.from_function(\n",
    "    func=consulta_sql,\n",
    "    name=\"consulta_sql\",\n",
    "    description=consulta_sql.__doc__.strip(),\n",
    "    args_schema=ConsultaSQLInput\n",
    ")\n",
    "\n",
    "\n",
    "inspect_tool = StructuredTool.from_function(\n",
    "    func=inspecionar_dataframe,\n",
    "    name='inspecionar_dataframe',\n",
    "    description=inspecionar_dataframe.__doc__.strip(),\n",
    "    args_schema=InspecionarDataFrameInput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pichau\\AppData\\Local\\Temp\\ipykernel_14432\\573878417.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  main_memory = ConversationBufferWindowMemory(\n",
      "C:\\Users\\Pichau\\AppData\\Local\\Temp\\ipykernel_14432\\573878417.py:11: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent_carregar = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "# 4) Criando os Sub-Agentes com memória\n",
    "# A memória é adicionada para cada sub-agente e para o agente principal\n",
    "##########main_memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "main_memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    k=5 # Mantenha as últimas 5 interações (5 pares de HumanMessage/AIMessage)\n",
    ")\n",
    "\n",
    "# Sub-Agente para Carregamento de Arquivos\n",
    "agent_carregar = initialize_agent(\n",
    "    tools=[zip_tool],\n",
    "    llm=llm,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS, # Usar OPENAI_FUNCTIONS para StructuredTool\n",
    "    verbose=False,\n",
    "    handle_parsing_errors=True,\n",
    "    memory=main_memory, # Adicionando memória\n",
    "    agent_kwargs={\n",
    "        \"system_message\": \"Você é um agente especialista em carregar e extrair arquivos ZIP que contêm CSVs. Use a ferramenta 'carregar_arquivos'.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sub-Agente para Consulta SQL (com a nova ferramenta e prompt aprimorado)\n",
    "agent_sql = initialize_agent(\n",
    "    tools=[query_tool, inspect_tool], # Adicione a ferramenta de inspeção aqui\n",
    "    llm=llm,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS, # Usar OPENAI_FUNCTIONS para StructuredTool\n",
    "    verbose=False,\n",
    "    handle_parsing_errors=True, \n",
    "    memory=main_memory, # Adicionando memória\n",
    "    agent_kwargs={\n",
    "        \"system_message\": \"\"\"Você é um agente especialista em executar consultas SQL no DataFrame global.\n",
    "        A partir das instruções recebidas use a ferramenta 'consulta_sql' e Formule as consultas SQL na tabela 'df_global' de maneira adequada.     \n",
    "        **Importante:** Ao formular SELECTs na tabela 'df_global', NUNCA use aspas nas colunas. Ex: Use `SELECT COLUNA` e não `SELECT \"COLUNA\"` ou `SELECT 'COLUNA'`.\"\"\"\n",
    "        \n",
    "        # **Instruções Importantes:**\n",
    "        #0. **Sobre seleção de colunas:** Garanta que na seleção de colunas \"SELECT\" nenhum tipo de aspas seja utilizado (nem simples, nem duplas)\n",
    "        #1.  **Operações Numéricas:** Ao lidar com operações como 'maior valor', 'menor valor', 'soma', 'média', etc., garanta que a coluna referenciada na consulta SQL seja tratada como um valor numérico. Se a coluna for originalmente texto ou string, use `CAST(coluna AS REAL)` ou `CAST(coluna AS INTEGER)` no SQL para conversão explícita, se necessário.\n",
    "        #2.  **Verificação de Dados:** Se você tiver dúvidas sobre os nomes das colunas, seus tipos de dados, ou para verificar se o DataFrame está carregado, **utilize a ferramenta 'inspecionar_dataframe' antes de formular a consulta SQL**.\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5) Definição das Ferramentas para o Agente Principal (que são os Sub-Agentes)\n",
    "\n",
    "# Use funções wrapper para adaptar o input para os sub-agentes\n",
    "def wrapper_carregar_arquivos_agent(arquivo_zip_path: str) -> str:\n",
    "    # O sub-agente agent_carregar.run espera um dicionário com a chave 'input'\n",
    "    return agent_carregar.run(input=arquivo_zip_path) # Ou {'input': arquivo_zip_path} dependendo da versão exata\n",
    "\n",
    "def wrapper_consulta_sql_agent(query: str) -> str:\n",
    "    # O sub-agente agent_sql.run espera um dicionário com a chave 'input'\n",
    "    return agent_sql.run(input=query)\n",
    "\n",
    "# Ferramentas do agente principal agora apontam para as funções wrapper\n",
    "tool_carregar_arquivos_agent = StructuredTool.from_function(\n",
    "    func=wrapper_carregar_arquivos_agent, # Use a função wrapper aqui\n",
    "    name=\"agente_carregar_arquivos\",\n",
    "    description=\"Útil para extrair, descompactar, abrir ou carregar arquivos de dados no formato .zip. Sempre forneça o caminho completo do arquivo ZIP. Esta deve ser a primeira ferramenta usada se o DataFrame não estiver carregado.\",\n",
    "    args_schema=CarregarArquivosInput\n",
    ")\n",
    "\n",
    "tool_consulta_sql_agent = StructuredTool.from_function(\n",
    "    func=wrapper_consulta_sql_agent, # Use a função wrapper aqui\n",
    "    name=\"agente_consulta_sql\",\n",
    "    description=\"Útil para realizar consultas, análises ou extrair informações do DataFrame já carregado usando SQL (ex: 'qual o valor', 'liste', 'some', 'agrupe por', 'maior', 'menor', 'média'). Esta ferramenta é usada após o carregamento do arquivo. Se houver dúvidas sobre colunas ou tipos, ela pode inspecionar o DataFrame.\",\n",
    "    args_schema=ConsultaSQLInput\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Inicializando o Agente Principal (Usando create_openai_tools_agent)\n",
    "# A memória é passada através do MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"Você é um analista de dados experiente.\n",
    "        Entenda a necessidade do usuário e atribua a tarefa para o agente especialista mais adequado.\n",
    "        \n",
    "        **Fluxo de Trabalho Prioritário:**\n",
    "        1.  **Carregamento:** Se o DataFrame global (df_global) não foi carregado OU o usuário explicitamente pede para abrir/extrair um arquivo ZIP, use 'agente_carregar_arquivos'.\n",
    "        2.  **Consulta/Análise:** Uma vez que o DataFrame esteja carregado e o usuário faça perguntas que exijam análise de dados (ex: \"qual o valor\", \"liste\", \"some\", \"agrupe por\", \"maior\", \"menor\", \"média\"), use 'agente_consulta_sql'.\n",
    "        \n",
    "        *IMPORTANTE** Sempre mantenha o contexto da conversa. Se um arquivo já foi carregado, não peça para carregá-lo novamente para perguntas subsequentes sobre os mesmos dados.\n",
    "        \"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"), # Para passar o histórico de conversas\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # Para o raciocínio do agente\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Ferramentas para o agente principal (os sub-agentes encapsulados)\n",
    "main_agent_tools = [tool_carregar_arquivos_agent, tool_consulta_sql_agent] # Adicionar tool_inspecionar_dataframe_agent aqui se for um sub-agente separado\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=create_openai_tools_agent(llm, main_agent_tools, prompt), # Crie o agente usando a nova forma\n",
    "    tools=main_agent_tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cenário 1: Carregar arquivo e fazer primeira consulta ---\n",
      "\n",
      "Usuário: Extraia o arquivo localizado em: 'C:\\Users\\Pichau\\Downloads\\teste\\202401_NFs.zip'. Qual o menor valor da coluna 'VALOR TOTAL'?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `agente_carregar_arquivos` with `{'arquivo_zip_path': 'C:\\\\Users\\\\Pichau\\\\Downloads\\\\teste\\\\202401_NFs.zip'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mThe .zip file contains a DataFrame with 565 rows and 30 columns. The columns are related to invoice details, including access key, model, series, number, operation nature, emission date, recent event, issuer and recipient details, operation destination, and product/service descriptions.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `agente_consulta_sql` with `{'query': 'SELECT MIN(`VALOR TOTAL`) AS menor_valor_total FROM df_global'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mThe minimum value in the \"VALOR_TOTAL\" column is 2.18.\u001b[0m\u001b[32;1m\u001b[1;3mO menor valor na coluna \"VALOR TOTAL\" é 2,18.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Resposta: O menor valor na coluna \"VALOR TOTAL\" é 2,18.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7) Testes\n",
    "# Inicializa o histórico de chat\n",
    "chat_history = []\n",
    "\n",
    "print(\"--- Cenário 1: Carregar arquivo e fazer primeira consulta ---\")\n",
    "# Usuário carrega o arquivo e faz uma pergunta. O agente deve carregar e depois consultar.\n",
    "user_input_1 = \"Extraia o arquivo localizado em: 'C:\\\\Users\\\\Pichau\\\\Downloads\\\\teste\\\\202401_NFs.zip'. Qual o menor valor da coluna 'VALOR TOTAL'?\"\n",
    "print(f\"\\nUsuário: {user_input_1}\")\n",
    "response_1 = agent_executor.invoke({\"input\": user_input_1, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=user_input_1), AIMessage(content=response_1[\"output\"])])\n",
    "print(f\"Resposta: {response_1['output']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cenário 2: Pergunta subsequente sem recarregar o arquivo ---\n",
      "\n",
      "Usuário: Qual o maior valor da coluna 'VALOR TOTAL'?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `agente_carregar_arquivos` with `{'arquivo_zip_path': 'C:\\\\Users\\\\Pichau\\\\Downloads\\\\teste\\\\202401_NFs.zip'}`\n",
      "responded: Parece que o DataFrame ainda não foi carregado. Vou começar extraindo o arquivo e, em seguida, realizarei a consulta para encontrar o maior valor da coluna 'VALOR TOTAL'.\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mThe ZIP file has been successfully extracted and contains a dataset with 565 rows and 30 columns. The columns available for analysis are listed above.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `agente_consulta_sql` with `{'query': 'SELECT MAX(`VALOR TOTAL`) AS maior_valor FROM df_global'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mThe maximum value in the \"VALOR_TOTAL\" column is 985,050.0.\u001b[0m\u001b[32;1m\u001b[1;3mO maior valor da coluna 'VALOR TOTAL' é 985,050.0.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Resposta: O maior valor da coluna 'VALOR TOTAL' é 985,050.0.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"--- Cenário 2: Pergunta subsequente sem recarregar o arquivo ---\")\n",
    "# O agente deve lembrar que o arquivo já está carregado e ir direto para a consulta.\n",
    "user_input_2 = \"Qual o maior valor da coluna 'VALOR TOTAL'?\"\n",
    "print(f\"\\nUsuário: {user_input_2}\")\n",
    "response_2 = agent_executor.invoke({\"input\": user_input_2, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=user_input_2), AIMessage(content=response_2[\"output\"])])\n",
    "print(f\"Resposta: {response_2['output']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cenário 3: Consulta complexa com potencial de tipo de dado ---\n",
      "\n",
      "Usuário: Qual a 'RAZÃO SOCIAL EMITENTE' que teve o segundo maior 'VALOR TOTAL'?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `agente_consulta_sql` with `{'query': 'SELECT `RAZÃO SOCIAL EMITENTE` FROM df_global ORDER BY `VALOR TOTAL` DESC LIMIT 1 OFFSET 1'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mThe second highest `VALOR TOTAL` corresponds to the `RAZÃO SOCIAL EMITENTE`: LABORATORIOS B.BRAUN S.A.\u001b[0m\u001b[32;1m\u001b[1;3mA \"RAZÃO SOCIAL EMITENTE\" que teve o segundo maior \"VALOR TOTAL\" é LABORATORIOS B.BRAUN S.A.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Resposta: A \"RAZÃO SOCIAL EMITENTE\" que teve o segundo maior \"VALOR TOTAL\" é LABORATORIOS B.BRAUN S.A.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Cenário 3: Consulta complexa com potencial de tipo de dado ---\")\n",
    "# Teste para o problema de \"Nota Fiscal\" sendo textual.\n",
    "# O LLM agora é instruído a tentar CAST se suspeitar de tipo de dado.\n",
    "user_input_3 = \"Qual a 'RAZÃO SOCIAL EMITENTE' que teve o segundo maior 'VALOR TOTAL'?\"\n",
    "print(f\"\\nUsuário: {user_input_3}\")\n",
    "response_3 = agent_executor.invoke({\"input\": user_input_3, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=user_input_3), AIMessage(content=response_3[\"output\"])])\n",
    "print(f\"Resposta: {response_3['output']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cenário 4: Solicitação de mapeamento de colunas com recuperação de erro ---\n",
      "\n",
      "Usuário: Qual 'Tipo de produto' teve maior 'quantidade'?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `agente_consulta_sql` with `{'query': 'SELECT \"Tipo de produto\", SUM(quantidade) as total_quantidade FROM df_global GROUP BY \"Tipo de produto\" ORDER BY total_quantidade DESC LIMIT 1;'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mThe product type with the highest total quantity is \"Outros livros, brochuras e impressos semelhantes\" with a total quantity of 68,684.0.\u001b[0m\u001b[32;1m\u001b[1;3mO \"Tipo de produto\" que teve a maior quantidade é \"Outros livros, brochuras e impressos semelhantes\" com um total de 68,684.0 unidades.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Resposta: O \"Tipo de produto\" que teve a maior quantidade é \"Outros livros, brochuras e impressos semelhantes\" com um total de 68,684.0 unidades.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Cenário 4: Solicitação de mapeamento de colunas com recuperação de erro ---\")\n",
    "# Força o uso do mapeamento se a primeira tentativa falhar.\n",
    "user_input_4 = \"Qual 'Tipo de produto' teve maior 'quantidade'?\"\n",
    "print(f\"\\nUsuário: {user_input_4}\")\n",
    "response_4 = agent_executor.invoke({\"input\": user_input_4, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=user_input_4), AIMessage(content=response_4[\"output\"])])\n",
    "print(f\"Resposta: {response_4['output']}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cenário 5: Outros testes\n",
      "\n",
      "Usuário: ola, qual a contagem de tokens destas perguntas?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mPara determinar a contagem de tokens das perguntas, raciocínio e resposta, eu precisaria de uma ferramenta específica para análise de tokens, o que não está disponível aqui. No entanto, posso ajudar com outras questões relacionadas a dados ou análises. Como posso ajudar você hoje?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Resposta: Para determinar a contagem de tokens das perguntas, raciocínio e resposta, eu precisaria de uma ferramenta específica para análise de tokens, o que não está disponível aqui. No entanto, posso ajudar com outras questões relacionadas a dados ou análises. Como posso ajudar você hoje?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Cenário 5: Outros testes\")\n",
    "# Força o uso do mapeamento se a primeira tentativa falhar.\n",
    "user_input_4 = \"ola, qual a contagem de tokens destas perguntas?\"\n",
    "print(f\"\\nUsuário: {user_input_4}\")\n",
    "response_4 = agent_executor.invoke({\"input\": user_input_4, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=user_input_4), AIMessage(content=response_4[\"output\"])])\n",
    "print(f\"Resposta: {response_4['output']}\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
