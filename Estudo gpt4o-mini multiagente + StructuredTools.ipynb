{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.3.25)\n",
      "Requirement already satisfied: openai in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.84.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.3.0)\n",
      "Requirement already satisfied: pandasql in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.7.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain_openai in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.3.65)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (2.11.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (3.12.7)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain_openai) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\Pichau\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# 0) Instalar todas as dependências necessárias\n",
    "%pip install langchain langchain-community openai pandas pandasql python-dotenv langchain_openai langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Importação das bibliotecas necessárias\n",
    "from langchain_openai import ChatOpenAI # Alterado para langchain_openai\n",
    "from langchain.agents import initialize_agent, AgentType # Usado para os sub-agentes\n",
    "from langchain_core.tools import StructuredTool # Para ferramentas com schema\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory # Importação da memória / definição de contexto para armazenamento da memória\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder # Para o agente principal\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent # Para o agente principal\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from pandasql import sqldf\n",
    "from io import TextIOWrapper\n",
    "import re\n",
    "from pydantic import BaseModel, Field # Necessário para StructuredTool\n",
    "from langchain_core.messages import HumanMessage, AIMessage \n",
    "\n",
    "# Variável global para armazenar o dataframe\n",
    "df_global = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Criação da função das ferramentas\n",
    "def carregar_arquivos(arquivo_zip_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Quando necessário extrai os arquivos do formato .zip (Extrair,Extract,Descompactar,Unzip,Abrir,Open,Deszipar,Decompress,Desempacotar,Unarchive,Desarquivar)\n",
    "    Após carregar e mesclar, informa o número de linhas, colunas e a lista das colunas disponíveis.\n",
    "    Exemplo: 'C:\\\\Users\\Pichau\\Downloads\\\\teste\\\\202401_NFs.zip'\n",
    "    \"\"\"\n",
    "    arquivo_zip_path = arquivo_zip_path.strip(\"'\\\"\")\n",
    "    global df_global # Declara que você vai modificar a variável global\n",
    "    grupo_csv = {}\n",
    "    \n",
    "    try:\n",
    "        with zipfile.ZipFile(arquivo_zip_path, 'r') as zip_ref:\n",
    "            csv_files_in_zip = [f for f in zip_ref.namelist() if f.endswith('.csv')]\n",
    "            \n",
    "            if len(csv_files_in_zip) < 2:\n",
    "                return f\"Erro: O arquivo ZIP '{os.path.basename(arquivo_zip_path)}' precisa conter pelo menos dois arquivos CSV para serem mesclados. Foram encontrados {len(csv_files_in_zip)} CSVs.\"\n",
    "            \n",
    "            for arquivo_csv in csv_files_in_zip:\n",
    "                with zip_ref.open(arquivo_csv) as arq:\n",
    "                    df = pd.read_csv(TextIOWrapper(arq,'utf-8', errors='ignore'))\n",
    "                    grupo_csv[arquivo_csv] = df\n",
    "            \n",
    "        sorted_keys = sorted(grupo_csv.keys())\n",
    "        primeiro_df = grupo_csv[sorted_keys[0]]\n",
    "        segundo_df = grupo_csv[sorted_keys[1]]\n",
    "        \n",
    "        # Tentativa de merge mais robusta: verificar se 'CHAVE DE ACESSO' existe em ambos\n",
    "        if 'CHAVE DE ACESSO' not in primeiro_df.columns or 'CHAVE DE ACESSO' not in segundo_df.columns:\n",
    "            return f\"Erro: A coluna 'CHAVE DE ACESSO' não foi encontrada em um ou ambos os arquivos CSV ({sorted_keys[0]}, {sorted_keys[1]}). Não foi possível mesclar.\"\n",
    "\n",
    "        df_global = pd.merge(primeiro_df, segundo_df, on='CHAVE DE ACESSO', how='left')\n",
    "        df_global.columns = df_global.columns.str.replace('_x$', '', regex=True)\n",
    "        \n",
    "        # Remover colunas _y de forma mais segura\n",
    "        cols_to_drop_y = [col for col in df_global.columns if col.endswith('_y')]\n",
    "        # Lista de colunas fixas a serem removidas (se existirem e terminarem com _y)\n",
    "        fixed_y_cols = ['MODELO_y','SÉRIE_y','NÚMERO_y','NATUREZA DA OPERAÇÃO_y','DATA EMISSÃO_y','CPF/CNPJ Emitente_y','RAZÃO SOCIAL EMITENTE_y','INSCRIÇÃO ESTADUAL EMITENTE_y','UF EMITENTE_y','MUNICÍPIO EMITENTE_y','CNPJ DESTINATÁRIO_y','NOME DESTINATÁRIO_y','UF DESTINATÁRIO_y','INDICADOR IE DESTINATÁRIO_y','DESTINO DA OPERAÇÃO_y','CONSUMIDOR FINAL_y','PRESENÇA DO COMPRADOR_y']\n",
    "        \n",
    "        # Filtra as colunas que realmente existem no DataFrame antes de dropar\n",
    "        cols_to_drop_final = [col for col in (cols_to_drop_y + fixed_y_cols) if col in df_global.columns]\n",
    "        df_global = df_global.drop(columns=list(set(cols_to_drop_final))) # Usar set para evitar duplicatas e converter para lista\n",
    "\n",
    "        colunas_originais = df_global.columns.tolist()\n",
    "    \n",
    "        mapeamento = {}\n",
    "        for col in colunas_originais:\n",
    "            col_nova = col.upper()\n",
    "            col_nova = re.sub(r\"[^A-Z0-9]\", \"_\", col_nova)  # Substitui espaços/símbolos por _\n",
    "            col_nova = re.sub(r\"_+\", \"_\", col_nova).strip(\"_\")  # Remove múltiplos _\n",
    "            mapeamento[col] = col_nova\n",
    "\n",
    "        df_global.columns = [mapeamento[col] for col in df_global.columns]\n",
    "       \n",
    "        return f\"Arquivos carregados e mesclados com sucesso! DataFrame pronto para análise com {len(df_global)} linhas e {len(df_global.columns)} colunas. As colunas disponíveis são: {list(df_global.columns)}.\"\n",
    "    except FileNotFoundError:\n",
    "        return f\"Erro: O arquivo ZIP '{arquivo_zip_path}' não foi encontrado. Verifique o caminho.\"\n",
    "    except zipfile.BadZipFile:\n",
    "        return f\"Erro: O arquivo '{arquivo_zip_path}' não é um arquivo ZIP válido ou está corrompido.\"\n",
    "    except Exception as e:\n",
    "        return f\"Erro inesperado ao carregar ou processar o arquivo ZIP: {e}. Detalhes: {str(e)}\"\n",
    "\n",
    "def consulta_sql(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Executa uma consulta SQL no DataFrame df_global.\n",
    "    Exemplo: SELECT SUM(\"VALOR TOTAL\") FROM df_global WHERE \"CPF/CNPJ DESTINATÁRIO\" = '12.345.678/0001-90'\n",
    "    \"\"\"\n",
    "    global df_global\n",
    "    if df_global is None:\n",
    "        return \"O DataFrame global ainda não foi carregado. Por favor, use a ferramenta 'carregar_arquivos' primeiro.\"\n",
    "\n",
    "    try:\n",
    "        # Prepara o ambiente para pandasql, garantindo que df_global esteja acessível\n",
    "        globals_dict = {'df_global': df_global}\n",
    "\n",
    "        result = sqldf(query, globals_dict)\n",
    "        if result.empty:\n",
    "            return \"A consulta SQL não retornou resultados.\"\n",
    "        return result.to_string(index=False)\n",
    "    except Exception as e:\n",
    "        # Mensagem de erro mais detalhada\n",
    "        cols_info = \"\"\n",
    "        if df_global is not None:\n",
    "            cols_info = f\"\\nColunas disponíveis: {list(df_global.columns)}.\"\n",
    "            # Tentar inferir se o erro é de tipo de dado ou coluna inexistente\n",
    "            if \"no such column\" in str(e).lower():\n",
    "                cols_info += \" Verifique se os nomes das colunas na sua consulta estão corretos e correspondem exatamente aos nomes listados.\"\n",
    "            elif \"no such table\" in str(e).lower() and 'df_global' not in query.lower():\n",
    "                cols_info += \" Lembre-se de referenciar o DataFrame como 'df_global' na sua consulta SQL.\"\n",
    "            elif \"could not convert string to float\" in str(e).lower() or \"numeric\" in str(e).lower():\n",
    "                 cols_info += \" Parece que você está tentando realizar uma operação numérica em uma coluna não numérica. Considere usar CAST() no SQL ou verificar o tipo da coluna com 'inspecionar_dataframe'.\"\n",
    "\n",
    "        return f\"Erro na consulta SQL: {e}.{cols_info}\"\n",
    "    \n",
    "\n",
    "def inspecionar_dataframe(args: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Retorna informações sobre o DataFrame global, incluindo as primeiras linhas,\n",
    "    nomes das colunas e seus respectivos tipos de dados (dtypes).\n",
    "    Útil para depurar consultas SQL, entender a estrutura dos dados ou verificar se o DataFrame foi carregado.\n",
    "    Não precisa de argumentos específicos, apenas chame a ferramenta para inspecionar.\n",
    "    \"\"\"\n",
    "    global df_global\n",
    "    if df_global is None:\n",
    "        return \"O DataFrame global ainda não foi carregado. Por favor, use a ferramenta 'carregar_arquivos' primeiro.\"\n",
    "    \n",
    "    info = f\"DataFrame carregado com {len(df_global)} linhas e {len(df_global.columns)} colunas.\\n\"\n",
    "    info += \"Colunas e Tipos de Dados:\\n\"\n",
    "    for col, dtype in df_global.dtypes.items():\n",
    "        info += f\"- '{col}': {dtype}\\n\"\n",
    "    info += \"\\nPrimeiras 5 linhas:\\n\"\n",
    "    # Adicionando um fallback caso o dataframe esteja vazio\n",
    "    if not df_global.empty:\n",
    "        info += df_global.head().to_string()\n",
    "    else:\n",
    "        info += \"O DataFrame está vazio.\"\n",
    "    return info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Escolhe o modelo selecionado e configuração de temperatura\n",
    "load_dotenv() # carrega o segredo\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    #model=\"openai/gpt-4o-mini\", # Ou \"gpt-3.5-turbo\"\n",
    "    #base_url=\"https://openrouter.ai/api/v1\",\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.25,\n",
    "    #api_key = os.getenv(\"open_key\"),\n",
    "    api_key = os.getenv(\"api_key\"),\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# --- Definição dos Schemas de Entrada para StructuredTool --- Essas classes, construídas com o Pydantic (BaseModel, Field), \n",
    "# são cruciais para a comunicação entre o LLM e suas ferramentas. \n",
    "# elas atuam como um \"contrato\" para cada ferramenta\n",
    "class CarregarArquivosInput(BaseModel):\n",
    "    arquivo_zip_path: str = Field(description=\"O caminho completo para o arquivo .zip a ser carregado.\")\n",
    "\n",
    "class ConsultaSQLInput(BaseModel):\n",
    "    query: str = Field(description=\"A consulta SQL a ser executada no DataFrame global (df_global).\")\n",
    "\n",
    "class InspecionarDataFrameInput(BaseModel):\n",
    "    args: str = Field(description=\"Argumento genérico, não é usado diretamente pela função, apenas para conformidade com StructuredTool.\")\n",
    "\n",
    "\n",
    "### Definição dos Sub-Agentes\n",
    "\n",
    "# Ferramentas específicas para cada sub-agente usando StructuredTool\n",
    "zip_tool = StructuredTool.from_function(\n",
    "    func=carregar_arquivos,\n",
    "    name=\"carregar_arquivos\",\n",
    "    description=carregar_arquivos.__doc__.strip(),\n",
    "    args_schema=CarregarArquivosInput\n",
    ")\n",
    "\n",
    "query_tool = StructuredTool.from_function(\n",
    "    func=consulta_sql,\n",
    "    name=\"consulta_sql\",\n",
    "    description=consulta_sql.__doc__.strip(),\n",
    "    args_schema=ConsultaSQLInput\n",
    ")\n",
    "\n",
    "\n",
    "inspect_tool = StructuredTool.from_function(\n",
    "    func=inspecionar_dataframe,\n",
    "    name='inspecionar_dataframe',\n",
    "    description=inspecionar_dataframe.__doc__.strip(),\n",
    "    args_schema=InspecionarDataFrameInput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pichau\\AppData\\Local\\Temp\\ipykernel_6856\\573878417.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  main_memory = ConversationBufferWindowMemory(\n",
      "C:\\Users\\Pichau\\AppData\\Local\\Temp\\ipykernel_6856\\573878417.py:11: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent_carregar = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "# 4) Criando os Sub-Agentes com memória\n",
    "# A memória é adicionada para cada sub-agente e para o agente principal\n",
    "##########main_memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "main_memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    k=5 # Mantenha as últimas 5 interações (5 pares de HumanMessage/AIMessage)\n",
    ")\n",
    "\n",
    "# Sub-Agente para Carregamento de Arquivos\n",
    "agent_carregar = initialize_agent(\n",
    "    tools=[zip_tool],\n",
    "    llm=llm,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS, # Usar OPENAI_FUNCTIONS para StructuredTool\n",
    "    verbose=False,\n",
    "    handle_parsing_errors=True,\n",
    "    memory=main_memory, # Adicionando memória\n",
    "    agent_kwargs={\n",
    "        \"system_message\": \"Você é um agente especialista em carregar e extrair arquivos ZIP que contêm CSVs. Use a ferramenta 'carregar_arquivos'.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sub-Agente para Consulta SQL (com a nova ferramenta e prompt aprimorado)\n",
    "agent_sql = initialize_agent(\n",
    "    tools=[query_tool, inspect_tool], # Adicione a ferramenta de inspeção aqui\n",
    "    llm=llm,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS, # Usar OPENAI_FUNCTIONS para StructuredTool\n",
    "    verbose=False,\n",
    "    handle_parsing_errors=True, \n",
    "    memory=main_memory, # Adicionando memória\n",
    "    agent_kwargs={\n",
    "        \"system_message\": \"\"\"Você é um agente especialista em executar consultas SQL no DataFrame global.\n",
    "        A partir das instruções recebidas use a ferramenta 'consulta_sql' e Formule as consultas SQL na tabela 'df_global' de maneira adequada.     \n",
    "        **Importante:** Ao formular SELECTs na tabela 'df_global', NUNCA use aspas nas colunas. Ex: Use `SELECT COLUNA` e não `SELECT \"COLUNA\"` ou `SELECT 'COLUNA'`.\"\"\"\n",
    "        \n",
    "        # **Instruções Importantes:**\n",
    "        #0. **Sobre seleção de colunas:** Garanta que na seleção de colunas \"SELECT\" nenhum tipo de aspas seja utilizado (nem simples, nem duplas)\n",
    "        #1.  **Operações Numéricas:** Ao lidar com operações como 'maior valor', 'menor valor', 'soma', 'média', etc., garanta que a coluna referenciada na consulta SQL seja tratada como um valor numérico. Se a coluna for originalmente texto ou string, use `CAST(coluna AS REAL)` ou `CAST(coluna AS INTEGER)` no SQL para conversão explícita, se necessário.\n",
    "        #2.  **Verificação de Dados:** Se você tiver dúvidas sobre os nomes das colunas, seus tipos de dados, ou para verificar se o DataFrame está carregado, **utilize a ferramenta 'inspecionar_dataframe' antes de formular a consulta SQL**.\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5) Definição das Ferramentas para o Agente Principal (que são os Sub-Agentes)\n",
    "\n",
    "# Use funções wrapper para adaptar o input para os sub-agentes\n",
    "def wrapper_carregar_arquivos_agent(arquivo_zip_path: str) -> str:\n",
    "    # O sub-agente agent_carregar.run espera um dicionário com a chave 'input'\n",
    "    return agent_carregar.run(input=arquivo_zip_path) # Ou {'input': arquivo_zip_path} dependendo da versão exata\n",
    "\n",
    "def wrapper_consulta_sql_agent(query: str) -> str:\n",
    "    # O sub-agente agent_sql.run espera um dicionário com a chave 'input'\n",
    "    return agent_sql.run(input=query)\n",
    "\n",
    "# Ferramentas do agente principal agora apontam para as funções wrapper\n",
    "tool_carregar_arquivos_agent = StructuredTool.from_function(\n",
    "    func=wrapper_carregar_arquivos_agent, # Use a função wrapper aqui\n",
    "    name=\"agente_carregar_arquivos\",\n",
    "    description=\"Útil para extrair, descompactar, abrir ou carregar arquivos de dados no formato .zip. Sempre forneça o caminho completo do arquivo ZIP. Esta deve ser a primeira ferramenta usada se o DataFrame não estiver carregado.\",\n",
    "    args_schema=CarregarArquivosInput\n",
    ")\n",
    "\n",
    "tool_consulta_sql_agent = StructuredTool.from_function(\n",
    "    func=wrapper_consulta_sql_agent, # Use a função wrapper aqui\n",
    "    name=\"agente_consulta_sql\",\n",
    "    description=\"Útil para realizar consultas, análises ou extrair informações do DataFrame já carregado usando SQL (ex: 'qual o valor', 'liste', 'some', 'agrupe por', 'maior', 'menor', 'média'). Esta ferramenta é usada após o carregamento do arquivo. Se houver dúvidas sobre colunas ou tipos, ela pode inspecionar o DataFrame.\",\n",
    "    args_schema=ConsultaSQLInput\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Inicializando o Agente Principal (Usando create_openai_tools_agent)\n",
    "# A memória é passada através do MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"Você é um analista de dados experiente.\n",
    "        Entenda a necessidade do usuário e atribua a tarefa para o agente especialista mais adequado.\n",
    "        \n",
    "        **Fluxo de Trabalho Prioritário:**\n",
    "        1.  **Carregamento:** Se o DataFrame global (df_global) não foi carregado OU o usuário explicitamente pede para abrir/extrair um arquivo ZIP, use 'agente_carregar_arquivos'.\n",
    "        2.  **Consulta/Análise:** Uma vez que o DataFrame esteja carregado e o usuário faça perguntas que exijam análise de dados (ex: \"qual o valor\", \"liste\", \"some\", \"agrupe por\", \"maior\", \"menor\", \"média\"), use 'agente_consulta_sql'.\n",
    "        \n",
    "        *IMPORTANTE** Sempre mantenha o contexto da conversa. Se um arquivo já foi carregado, não peça para carregá-lo novamente para perguntas subsequentes sobre os mesmos dados.\n",
    "        \"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"), # Para passar o histórico de conversas\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # Para o raciocínio do agente\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Ferramentas para o agente principal (os sub-agentes encapsulados)\n",
    "main_agent_tools = [tool_carregar_arquivos_agent, tool_consulta_sql_agent] # Adicionar tool_inspecionar_dataframe_agent aqui se for um sub-agente separado\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=create_openai_tools_agent(llm, main_agent_tools, prompt), # Crie o agente usando a nova forma\n",
    "    tools=main_agent_tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cenário 1: Carregar arquivo e fazer primeira consulta ---\n",
      "\n",
      "Usuário: Extraia o arquivo localizado em: 'C:\\Users\\Pichau\\Downloads\\teste\\202401_NFs.zip'. Qual o menor valor da coluna 'VALOR TOTAL'?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `agente_carregar_arquivos` with `{'arquivo_zip_path': 'C:\\\\Users\\\\Pichau\\\\Downloads\\\\teste\\\\202401_NFs.zip'}`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pichau\\AppData\\Local\\Temp\\ipykernel_6856\\2583462552.py:6: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return agent_carregar.run(input=arquivo_zip_path) # Ou {'input': arquivo_zip_path} dependendo da versão exata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mThe extracted and loaded file contains 565 lines and 30 columns. The available columns are: \n",
      "\n",
      "1. CHAVE_DE_ACESSO\n",
      "2. MODELO\n",
      "3. S_RIE\n",
      "4. N_MERO\n",
      "5. NATUREZA_DA_OPERA_O\n",
      "6. DATA_EMISS_O\n",
      "7. EVENTO_MAIS_RECENTE\n",
      "8. DATA_HORA_EVENTO_MAIS_RECENTE\n",
      "9. CPF_CNPJ_EMITENTE\n",
      "10. RAZ_O_SOCIAL_EMITENTE\n",
      "11. INSCRI_O_ESTADUAL_EMITENTE\n",
      "12. UF_EMITENTE\n",
      "13. MUNIC_PIO_EMITENTE\n",
      "14. CNPJ_DESTINAT_RIO\n",
      "15. NOME_DESTINAT_RIO\n",
      "16. UF_DESTINAT_RIO\n",
      "17. INDICADOR_IE_DESTINAT_RIO\n",
      "18. DESTINO_DA_OPERA_O\n",
      "19. CONSUMIDOR_FINAL\n",
      "20. PRESEN_A_DO_COMPRADOR\n",
      "21. VALOR_NOTA_FISCAL\n",
      "22. N_MERO_PRODUTO\n",
      "23. DESCRI_O_DO_PRODUTO_SERVI_O\n",
      "24. C_DIGO_NCM_SH\n",
      "25. NCM_SH_TIPO_DE_PRODUTO\n",
      "26. CFOP\n",
      "27. QUANTIDADE\n",
      "28. UNIDADE\n",
      "29. VALOR_UNIT_RIO\n",
      "30. VALOR_TOTAL\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `agente_consulta_sql` with `{'query': 'SELECT MIN(VALOR_TOTAL) AS menor_valor_total FROM df_global'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mThe minimum value in the \"VALOR_TOTAL\" column is 2.18.\u001b[0m\u001b[32;1m\u001b[1;3mO menor valor na coluna \"VALOR TOTAL\" é 2.18.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Resposta: O menor valor na coluna \"VALOR TOTAL\" é 2.18.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7) Testes\n",
    "# Inicializa o histórico de chat\n",
    "chat_history = []\n",
    "\n",
    "print(\"--- Cenário 1: Carregar arquivo e fazer primeira consulta ---\")\n",
    "# Usuário carrega o arquivo e faz uma pergunta. O agente deve carregar e depois consultar.\n",
    "user_input_1 = \"Extraia o arquivo localizado em: 'C:\\\\Users\\\\Pichau\\\\Downloads\\\\teste\\\\202401_NFs.zip'. Qual o menor valor da coluna 'VALOR TOTAL'?\"\n",
    "print(f\"\\nUsuário: {user_input_1}\")\n",
    "response_1 = agent_executor.invoke({\"input\": user_input_1, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=user_input_1), AIMessage(content=response_1[\"output\"])])\n",
    "print(f\"Resposta: {response_1['output']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cenário 2: Pergunta subsequente sem recarregar o arquivo ---\n",
      "\n",
      "Usuário: Qual o maior valor da coluna 'VALOR TOTAL'?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `agente_carregar_arquivos` with `{'arquivo_zip_path': 'C:\\\\Users\\\\Pichau\\\\Downloads\\\\teste\\\\202401_NFs.zip'}`\n",
      "responded: Para responder a essa pergunta, preciso primeiro extrair o arquivo localizado em 'C:\\Users\\Pichau\\Downloads\\teste\\202401_NFs.zip'. Vou fazer isso agora.\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mThe extracted DataFrame has 565 rows and 30 columns. The columns are: ['CHAVE_DE_ACESSO', 'MODELO', 'S_RIE', 'N_MERO', 'NATUREZA_DA_OPERA_O', 'DATA_EMISS_O', 'EVENTO_MAIS_RECENTE', 'DATA_HORA_EVENTO_MAIS_RECENTE', 'CPF_CNPJ_EMITENTE', 'RAZ_O_SOCIAL_EMITENTE', 'INSCRI_O_ESTADUAL_EMITENTE', 'UF_EMITENTE', 'MUNIC_PIO_EMITENTE', 'CNPJ_DESTINAT_RIO', 'NOME_DESTINAT_RIO', 'UF_DESTINAT_RIO', 'INDICADOR_IE_DESTINAT_RIO', 'DESTINO_DA_OPERA_O', 'CONSUMIDOR_FINAL', 'PRESEN_A_DO_COMPRADOR', 'VALOR_NOTA_FISCAL', 'N_MERO_PRODUTO', 'DESCRI_O_DO_PRO\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `agente_consulta_sql` with `{'query': 'SELECT MAX(\"VALOR TOTAL\") AS maior_valor FROM df_global'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mThe maximum value in the \"VALOR_TOTAL\" column is 985,050.0.\u001b[0m\u001b[32;1m\u001b[1;3mO maior valor na coluna \"VALOR TOTAL\" é 985,050.0.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Resposta: O maior valor na coluna \"VALOR TOTAL\" é 985,050.0.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"--- Cenário 2: Pergunta subsequente sem recarregar o arquivo ---\")\n",
    "# O agente deve lembrar que o arquivo já está carregado e ir direto para a consulta.\n",
    "user_input_2 = \"Qual o maior valor da coluna 'VALOR TOTAL'?\"\n",
    "print(f\"\\nUsuário: {user_input_2}\")\n",
    "response_2 = agent_executor.invoke({\"input\": user_input_2, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=user_input_2), AIMessage(content=response_2[\"output\"])])\n",
    "print(f\"Resposta: {response_2['output']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cenário 3: Consulta complexa com potencial de tipo de dado ---\n",
      "\n",
      "Usuário: Qual a 'RAZÃO SOCIAL EMITENTE' que teve o segundo maior 'VALOR TOTAL'?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `agente_consulta_sql` with `{'query': 'SELECT \"RAZÃO SOCIAL EMITENTE\" FROM df_global ORDER BY \"VALOR TOTAL\" DESC LIMIT 1 OFFSET 1'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mThe \"RAZÃO SOCIAL EMITENTE\" with the second highest \"VALOR TOTAL\" is \"LABORATORIOS B.BRAUN S.A\".\u001b[0m\u001b[32;1m\u001b[1;3mA \"RAZÃO SOCIAL EMITENTE\" que teve o segundo maior \"VALOR TOTAL\" é \"LABORATORIOS B.BRAUN S.A\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Resposta: A \"RAZÃO SOCIAL EMITENTE\" que teve o segundo maior \"VALOR TOTAL\" é \"LABORATORIOS B.BRAUN S.A\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Cenário 3: Consulta complexa com potencial de tipo de dado ---\")\n",
    "# Teste para o problema de \"Nota Fiscal\" sendo textual.\n",
    "# O LLM agora é instruído a tentar CAST se suspeitar de tipo de dado.\n",
    "user_input_3 = \"Qual a 'RAZÃO SOCIAL EMITENTE' que teve o segundo maior 'VALOR TOTAL'?\"\n",
    "print(f\"\\nUsuário: {user_input_3}\")\n",
    "response_3 = agent_executor.invoke({\"input\": user_input_3, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=user_input_3), AIMessage(content=response_3[\"output\"])])\n",
    "print(f\"Resposta: {response_3['output']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cenário 4: Solicitação de mapeamento de colunas com recuperação de erro ---\n",
      "\n",
      "Usuário: Qual 'Tipo de produto' teve maior 'quantidade'?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `agente_consulta_sql` with `{'query': 'SELECT \"Tipo de produto\", SUM(quantidade) as total_quantidade FROM df_global GROUP BY \"Tipo de produto\" ORDER BY total_quantidade DESC LIMIT 1'}`\n",
      "responded: Para responder a essa pergunta, preciso realizar uma consulta no DataFrame carregado. Vou fazer isso agora.\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mThe product type with the highest total quantity is \"Outros livros, brochuras e impressos semelhantes\" with a total quantity of 68,684.\u001b[0m\u001b[32;1m\u001b[1;3mO \"Tipo de produto\" que teve a maior quantidade é \"Outros livros, brochuras e impressos semelhantes\", com uma quantidade total de 68,684.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Resposta: O \"Tipo de produto\" que teve a maior quantidade é \"Outros livros, brochuras e impressos semelhantes\", com uma quantidade total de 68,684.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Cenário 4: Solicitação de mapeamento de colunas com recuperação de erro ---\")\n",
    "# Força o uso do mapeamento se a primeira tentativa falhar.\n",
    "user_input_4 = \"Qual 'Tipo de produto' teve maior 'quantidade'?\"\n",
    "print(f\"\\nUsuário: {user_input_4}\")\n",
    "response_4 = agent_executor.invoke({\"input\": user_input_4, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=user_input_4), AIMessage(content=response_4[\"output\"])])\n",
    "print(f\"Resposta: {response_4['output']}\\n\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
