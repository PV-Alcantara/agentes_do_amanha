{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7d74ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bibliotecas do agente \n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.agents import initialize_agent, AgentType, AgentExecutor # Usado para os sub-agentes\n",
    "from langchain_core.tools import StructuredTool # Para ferramentas com schema\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory # Importação da memória / definição de contexto para armazenamento da memória\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder # Para o agente principal\n",
    "from pydantic import BaseModel, Field # Necessário para StructuredTool\n",
    "from langchain_core.messages import HumanMessage, AIMessage \n",
    "\n",
    "\n",
    "#Funções diversas em Python\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandasql import sqldf\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "#importações para RAG\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_chroma import Chroma #banco vetorial\n",
    "import shutil \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7def8e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Documentos carregados. Número de documentos: 1\n",
      "DEBUG: Número de chunks criados: 4\n",
      "DEBUG: Primeiro chunk: 1) REGRAS DE SINTAXE \n",
      " **NÃO INCLUA ASPAS TRIPLAS (```) OU QUALQUER OUTRO DELIMITADOR DE CÓDIGO NO A...\n"
     ]
    }
   ],
   "source": [
    "#1) Criação da pasta onde estão as informações\n",
    "docs_folder = \"docs\"\n",
    "csv_file_path = os.path.join(docs_folder, \"dataset.csv\")\n",
    "txt_file_path = os.path.join(docs_folder, \"regras.txt\")\n",
    "\n",
    "#carregar os documentos\n",
    "loader = TextLoader(txt_file_path, encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "print(f\"DEBUG: Documentos carregados. Número de documentos: {len(documents)}\")\n",
    "\n",
    "# Dividir documentos em chunks (pedaços menores)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"DEBUG: Número de chunks criados: {len(splits)}\")\n",
    "print(f\"DEBUG: Primeiro chunk: {splits[0].page_content[:100]}...\")\n",
    "\n",
    "embeddings_model = 'nomic-embed-text'\n",
    "embeddings = OllamaEmbeddings(model=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa45df8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Removendo diretório existente do ChromaDB para forçar atualização: './chroma_db_agente_local'\n",
      "DEBUG: Diretório do ChromaDB removido com sucesso.\n",
      "DEBUG: Criando e preenchendo novo ChromaDB com documentos atualizados...\n",
      "DEBUG: Novo ChromaDB criado e preenchido com sucesso com dados atualizados!\n",
      "\n",
      "DEBUG: Número de chunks originais (splits): 4\n",
      "DEBUG: Número de documentos no ChromaDB: 4\n",
      "DEBUG: A contagem de documentos no ChromaDB corresponde ao número de splits.\n"
     ]
    }
   ],
   "source": [
    "# 2) Criação ou Carregamento do banco vetorial em chroma_db\n",
    "# Definindo um caminho de pasta específico para este projeto\n",
    "chroma_db_path = \"./chroma_db_agente_local\" \n",
    "vectorstore = None # Inicializa o vectorstore como None\n",
    "\n",
    "if os.path.exists(chroma_db_path):\n",
    "    print(f\"DEBUG: Removendo diretório existente do ChromaDB para forçar atualização: '{chroma_db_path}'\")\n",
    "    shutil.rmtree(chroma_db_path)\n",
    "    print(\"DEBUG: Diretório do ChromaDB removido com sucesso.\")\n",
    "\n",
    "print(\"DEBUG: Criando e preenchendo novo ChromaDB com documentos atualizados...\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=chroma_db_path\n",
    ")\n",
    "print(\"DEBUG: Novo ChromaDB criado e preenchido com sucesso com dados atualizados!\")\n",
    "\n",
    "# teste. Contagem de Documentos\n",
    "print(f\"\\nDEBUG: Número de chunks originais (splits): {len(splits)}\")\n",
    "try:\n",
    "    chroma_collection = vectorstore._collection\n",
    "    count_in_db = chroma_collection.count()\n",
    "    print(f\"DEBUG: Número de documentos no ChromaDB: {count_in_db}\")\n",
    "\n",
    "    if count_in_db == len(splits):\n",
    "        print(\"DEBUG: A contagem de documentos no ChromaDB corresponde ao número de splits.\")\n",
    "    else:\n",
    "        print(\"ALERTA: A contagem de documentos no ChromaDB NÃO corresponde ao número de splits. Pode haver um problema.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERRO: Não foi possível contar documentos no ChromaDB: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97944064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Dataset CSV carregado em DataFrame. Linhas: 765\n"
     ]
    }
   ],
   "source": [
    "#3) Criação de funções\n",
    "\n",
    "#criação do dataset\n",
    "df_info_dataset = None\n",
    "try:\n",
    "    df_info_dataset = pd.read_csv(csv_file_path)\n",
    "    print(f\"DEBUG: Dataset CSV carregado em DataFrame. Linhas: {len(df_info_dataset)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRO: O arquivo '{csv_file_path}' não foi encontrado.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERRO: Não foi possível carregar o CSV para DataFrame: {e}\")\n",
    "\n",
    "\n",
    "def buscar_documentos (query: str) -> str:\n",
    "    \"\"\"**PRIMEIRA ETAPA SEMPRE**\n",
    "    utilize essa ferramenta sempre no inicio da query para interpretar e entender as regras de negócio da pergunta do usuário.\n",
    "    essa ferramenta aceita a query em linguagem natural.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    retriever = vectorstore.as_retriever()\n",
    "    docs = retriever.invoke(query)\n",
    "    if docs:\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    else:\n",
    "        return \"Não encontrei informações relevantes nos documentos para esta consulta.\"\n",
    "    \n",
    "def sql_documentos (query_sql: str) -> str:\n",
    "    \"\"\"\n",
    "    ** SEGUNDA ETAPA SEMPRE**\n",
    "    Use esta ferramenta para **consultas quantitativas/qualitativas** dentro do dataset chamado 'df_info_dataset'\n",
    "    Esta ferramenta exige uma query onde o 'from' deve ser sempre 'df_info_dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    global df_info_dataset # Permite que a função acesse o df_data global\n",
    "    if df_info_dataset is None:\n",
    "        return \"Erro: O DataFrame 'df_info_dataset' não foi carregado. Não é possível executar consultas SQL.\"\n",
    "\n",
    "    colunas_originais = df_info_dataset.columns.tolist()\n",
    "    \n",
    "    # Cria dicionário de mapeamento simples\n",
    "    mapeamento = {}\n",
    "    for col in colunas_originais:\n",
    "        col_nova = col.upper()\n",
    "        col_nova = re.sub(r\"[^A-Z0-9]\", \"_\", col_nova)  # Substitui espaços/símbolos por _\n",
    "        col_nova = re.sub(r\"_+\", \"_\", col_nova).strip(\"_\")  # Remove múltiplos _\n",
    "        mapeamento[col] = col_nova\n",
    "\n",
    "    # Aplica o mapeamento\n",
    "    df_info_dataset.columns = [mapeamento[col] for col in df_info_dataset.columns]\n",
    "    \n",
    "\n",
    "    result = sqldf(query_sql, {'df_info_dataset': df_info_dataset}) # Mapeamento do dataset para o agente\n",
    "\n",
    "   \n",
    "    if not result.empty and len(result.columns) == 1 and len(result) == 1:\n",
    "        return str(result.iloc[0, 0]) # Retorna o valor da única célula como string\n",
    "    else:\n",
    "        return result.to_string(index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6866b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4) Criação de Schema\n",
    "class buscar_documentos_input(BaseModel):\n",
    "    query: str = Field(description=\"Consulta na documentaçao.\")\n",
    "\n",
    "class sql_documentos_input(BaseModel):\n",
    "    query_sql: str = Field(description=\"Consulta SQL no dataset df_info_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828cb531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5) Criação das ferramentas estruturadas\n",
    "buscar_tool = StructuredTool.from_function(\n",
    "    func=buscar_documentos,\n",
    "    name='buscar_documentos',\n",
    "    description=buscar_documentos.__doc__.strip(),\n",
    "    args_schema=buscar_documentos_input\n",
    ")\n",
    "\n",
    "query_tool = StructuredTool.from_function(\n",
    "    func=sql_documentos,\n",
    "    name='sql_documentos',\n",
    "    description=sql_documentos.__doc__.strip(),\n",
    "    args_schema=sql_documentos_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d622973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pichau\\AppData\\Local\\Temp\\ipykernel_6316\\2064540434.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  main_memory = ConversationBufferWindowMemory(\n"
     ]
    }
   ],
   "source": [
    "#6) Criação da memória dos agentes\n",
    "main_memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    k=5 # Mantenha as últimas 5 interações (5 pares de HumanMessage/AIMessage)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e0c9da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Agente configurado com sucesso para LLM local.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pichau\\AppData\\Local\\Temp\\ipykernel_6316\\1699356882.py:7: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent_executor = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "#7) Criação do Agente\n",
    "llm = OllamaLLM(model=\"qwen3\",\n",
    "                temperature=0.15,\n",
    "                max_tokens=500              \n",
    "                )\n",
    "\n",
    "agent_executor = initialize_agent(\n",
    "    tools=[query_tool, buscar_tool],\n",
    "    llm=llm,\n",
    "    agent_type=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    memory=main_memory, # Adicionando memória\n",
    "    agent_kwargs={\"system_message\": \"\"\"Você é um analista de dados focado em trazer os resultados de projetos.\n",
    "    Sempre utilize as ferramentas disponíveis para responder às perguntas do usuário.\n",
    "\n",
    "    **IMPORTANTE: Seu processo de pensamento deve seguir o formato ReAct (Thought, Action, Action Input, Observation, Thought, Final Answer).**\n",
    "\n",
    "    **Regras de Uso das Ferramentas:**\n",
    "                                \n",
    "    1.  SEMPRE SIGA O SEGUINTE FLUXO: I. buscar_tool (para entendimento das regras de negócio) -> II. query_tool (para confecçao da query quantitativa/qualitativa),\n",
    "                        \n",
    "        **Para perguntas de caráter quantitativo ou qualitativo que exigem análise de dados tabulares, agregação (COUNT, SUM, AVG), filtragem precisa ou qualquer cálculo sobre o dataset, VOCÊ DEVE usar a ferramenta 'query_tool'. Lembre-se que o nome da tabela no SQL é 'df_info_dataset'.**\n",
    "        * Ao usar a 'query_tool', o **Action Input DEVE CONTER APENAS a string SQL pura e NADA MAIS**.\n",
    "        * **NÃO INCLUA ASPAS TRIPLAS (```) OU QUALQUER OUTRO DELIMITADOR DE CÓDIGO NO Action Input para 'query_tool'.**\n",
    "        * **EXEMPLO CERTO:** Action Input: SELECT MARCA, SUM(CPV_UND) FROM df_info_dataset GROUP BY MARCA\n",
    "        * **EXEMPLO ERRADO:** Action Input: ```SELECT MARCA, SUM(CPV_UND) FROM df_info_dataset GROUP BY MARCA```\n",
    "                  \n",
    "        **Regras Específicas para Filtros de Data:**\n",
    "        * Ao filtrar por datas, **SEMPRE** utilize o formato `WHERE DATA >= 'YYYY-MM-DD' AND DATA <= 'YYYY-MM-DD'`.\n",
    "        * **NUNCA** utilize funções como `year(DATA) = YYYY` ou `month(DATA) = MM` ou qualquer outro método que não seja a comparação direta de datas no formato 'YYYY-MM-DD'.\n",
    "        * **EXEMPLO CERTO DE FILTRO DE DATA:** `WHERE DATA >='2025-05-01' AND DATA <= '2024-05-31'` \n",
    "        * **EXEMPLO ERRADO DE FILTRO DE DATA:** `WHERE year(DATA) = 2024 AND month(DATA) = 5`\n",
    "                   \n",
    "        * Exemplo de Thought para esta ferramenta: \"A pergunta é quantitativa, preciso usar query_tool para contar as marcas.\"\n",
    "        * Exemplo de Final Answer após usar esta ferramenta: \"Existem X marcas no dataset.\"\n",
    "                            \n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    ")\n",
    "print(\"DEBUG: Agente configurado com sucesso para LLM local.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5de7357c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Teste 1: Pergunta 1 ---\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<think>\n",
      "Okay, the user is asking how many brands exist in the file. First, I need to figure out the correct approach. The tools available are 'buscar_documentos' and 'sql_documentos'. According to the instructions, 'buscar_documentos' should be used first to understand the business rules or structure of the data. So I'll start by using that tool to get an overview.\n",
      "\n",
      "I'll input the query \"Quantas Marcas existem neste arquivo?\" into 'buscar_documentos'. Wait, but maybe the tool expects a natural language query. Let me try using 'buscar_documentos' with a query that asks about the number of brands. Alternatively, maybe just using a general query to understand the dataset structure. Let me check the tool's description again. The 'buscar_documentos' is for interpreting and understanding the business rules, so perhaps I should first use it to get information about the dataset's structure, like the columns and possible values. \n",
      "\n",
      "Alternatively, maybe the user's question is straightforward, and using 'sql_documentos' directly with a SQL query would be better. But the instructions say that 'sql_documentos' is for quantitative/qualitative queries and requires the 'from' clause to be 'df_info_dataset'. So if I can form a SQL query to count distinct brands, that would work. But first, I need to confirm the column name for brands. Maybe the column is named 'Marca' or 'Brand'. \n",
      "\n",
      "Wait, the user's question is in Portuguese, so the column might be named 'Marca'. To be safe, I should first use 'buscar_documentos' to understand the dataset's structure. Let me try that. \n",
      "\n",
      "Action: buscar_documentos\n",
      "Action Input: \"Quais são as colunas e as regras de negócio do dataset df_info_dataset?\"\n",
      "\n",
      "Then, based on the observation, I can determine the correct column name for brands and proceed to use 'sql_documentos' to count them.\n",
      "</think>\n",
      "\n",
      "Thought: Preciso primeiro entender a estrutura do dataset para identificar a coluna que representa as marcas.\n",
      "Action: buscar_documentos\n",
      "Action Input: \"Quais são as colunas e as regras de negócio do dataset df_info_dataset?\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m**EXEMPLO_3 CONVERSÃO, o usuário pode pedir a conversão de algum produto, esta regra consiste na soma de CPV_UND dividido pela soma de MDTR_UND, deve-se seguir o exemplo da estrutura abaixo e alterando as variáveis conforme necessidade:\n",
      "select sum(CPV_UND) / sum(MDTR_UND)\n",
      "from df_info_dataset\n",
      "group by Marca\n",
      "\n",
      "**EXEMPLO_4 TOTAL DE MARCAS, o usuário pode pedir o total de marcas distintas de algum produto, esta regra consiste na contagem distinta das marcas , deve-se seguir o exemplo da estrutura abaixo e alterando as variáveis conforme necessidade:\n",
      "select count(distinct(Marca))\n",
      "from df_info_dataset\n",
      "group by Marca\n",
      "\n",
      "\n",
      "3) ESTRUTURA DO DATASET, TERMOS & SINÔNIMOS do dataset 'df_info_dataset':\n",
      "* 'DATA': Tipo data (yyyy-mm-dd).\n",
      "* 'MARCA': Tipo texto (ex: 'LUTEAL (A4H)').\n",
      "* 'APRESENTACAO': Tipo texto (ex: 'LUTEAL CAP MOLE BLX60').\n",
      "* 'EAN': Tipo texto/numérico (código, ex: '7896658040894').\n",
      "* 'MDTR_UND': Tipo numérico (Unidades de MDTR).\n",
      "* 'CPV_UND': Tipo numérico (Unidades de CPV).\n",
      "\n",
      "1) REGRAS DE SINTAXE \n",
      " **NÃO INCLUA ASPAS TRIPLAS (```) OU QUALQUER OUTRO DELIMITADOR DE CÓDIGO NO Action Input para 'query_tool'.**\n",
      " **EXEMPLO CERTO:** Action Input: SELECT MARCA, SUM(CPV_UND) FROM df_info_dataset GROUP BY MARCA\n",
      " **EXEMPLO ERRADO:** Action Input: ```SELECT MARCA, SUM(CPV_UND) FROM df_info_dataset GROUP BY MARCA```\n",
      "                  \n",
      " **Regras Específicas para Filtros de Data:**\n",
      " * Ao filtrar por datas, **SEMPRE** utilize o formato `WHERE DATA >= 'YYYY-MM-DD' AND DATA <= 'YYYY-MM-DD'`.\n",
      " * **NUNCA** utilize funções como `year(DATA) = YYYY` ou `month(DATA) = MM` ou qualquer outro método que não seja a comparação direta de datas no formato 'YYYY-MM-DD'.\n",
      " * **EXEMPLO CERTO DE FILTRO DE DATA:** `WHERE DATA >='2025-05-01' AND DATA <= '2024-05-31'` \n",
      "\n",
      " * **EXEMPLO ERRADO DE FILTRO DE DATA:** `WHERE year(DATA) = 2024 AND month(DATA) = 5`\n",
      "\n",
      "* **EXEMPLO ERRADO DE FILTRO DE DATA:** `WHERE year(DATA) = 2024 AND month(DATA) = 5`\n",
      "\n",
      "\n",
      "2) REGRAS DE NEGOCIO\n",
      "**EXEMPLO_1 SOMA, o usuário pode pedir o valor total de uma marca então deve-se utilizar a ferramenta adequada para query, seguindo o exemplo da estrutura abaixo e alterando as variáveis conforme necessidade:\n",
      "select sum(CPV_UND) \n",
      "from df_info_dataset\n",
      "where Marca = 'MOTORE (A4H)'\n",
      "\n",
      "**EXEMPLO_2 MÉDIA, o usuário pode pedir a média de  períodos então deve-se utilizar a ferramenta adequada para query, seguindo o exemplo da estrutura abaixo e alterando as variáveis conforme necessidade:\n",
      "select avg(CPV_UND) \n",
      "from df_info_dataset\n",
      "where Data >= '2024-01-01' and Data <= '2024-12-31' and Marca = 'ALENIA (A4H)'\n",
      "\n",
      "**Sinônimos e Definições Adicionais:**\n",
      "* Marca pode ser também 'Produto'.\n",
      "* Data pode ser também 'Período, Data ou Ocorrência'.\n",
      "* Apresentacao pode ser também 'SKU'.\n",
      "* EAN pode ser também 'Código'.\n",
      "* CPV_UND pode ser também 'UNIDADES CPV','CPV_UNIDADES','UND_CAMPANHA','UNIDADES CAMPANHA','UNIDADES VENDIDAS'.\n",
      "* MDTR_UND pode ser também 'UNIDADES MDTR', 'MDTR_UNIDADES', 'MDTR'.\n",
      "\n",
      "* Caso o usuário já traga o nome da Marca, não se esqueça de adicionar o sulfixo ' (A4H)' caso ele não tenha escrito.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m<think>\n",
      "</think>\n",
      "\n",
      "Agora que sei que a coluna é chamada de \"Marca\", posso usar a ferramenta sql_documentos para contar o número de marcas distintas.\n",
      "Action: sql_documentos\n",
      "Action Input: select count(distinct(Marca)) from df_info_dataset\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m35\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m<think>\n",
      "</think>\n",
      "\n",
      "Final Answer: Existem 35 Marcas no arquivo.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Resposta: Existem 35 Marcas no arquivo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8) Testando o agente\n",
    "chat_history = []\n",
    "\n",
    "print(\"\\n--- Teste 1: Pergunta 1 ---\")\n",
    "user_input_1 = \"Quantas Marcas existem neste arquivo?\"\n",
    "response_1 = agent_executor.invoke({\"input\": user_input_1, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=user_input_1), AIMessage(content=response_1[\"output\"])])\n",
    "print(f\"Resposta: {response_1['output']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bd5808b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Teste 2: Pergunta de cálculo ---\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<think>\n",
      "Okay, let's tackle this question. The user wants to know which brand sold the most units, considering the total units sold. The dataset is called 'df_info_dataset'. \n",
      "\n",
      "First, I need to understand the structure of the data. Since the user mentioned \"unidades CPV\", I should check what columns are present. The 'buscar_documentos' tool can help me get an overview. I'll start by using that tool to understand the dataset's structure and the relevant columns.\n",
      "\n",
      "Action: buscar_documentos\n",
      "Action Input: \"Descrição do dataset df_info_dataset\"\n",
      "\n",
      "Wait, the user might not have provided the actual dataset description. Maybe I should ask for more details, but since I can't, I'll proceed with the tools available. Alternatively, perhaps the 'buscar_documentos' tool can help me infer the columns by querying the dataset. Let me try using it with a general query.\n",
      "\n",
      "Wait, the 'buscar_documentos' tool is for interpreting the business rules, so maybe I should use it to understand the relevant fields. For example, if the dataset has columns like 'marca' (brand), 'unidades_vendidas' (units sold), etc. \n",
      "\n",
      "Let me try using 'buscar_documentos' to get an overview of the dataset's structure. \n",
      "\n",
      "Action: buscar_documentos\n",
      "Action Input: \"Quais são as colunas do dataset df_info_dataset?\"\n",
      "\n",
      "Assuming the response includes columns like 'marca', 'unidades_vendidas', etc. Then, I can proceed to write an SQL query to sum the units per brand and find the maximum. \n",
      "\n",
      "Once I have the columns, the next step is to use 'sql_documentos' to execute the query. The SQL would group by brand, sum the units, and order in descending order to find the top seller. \n",
      "\n",
      "So, the steps are: first, understand the dataset structure with 'buscar_documentos', then use 'sql_documentos' to perform the aggregation and find the brand with the highest total units.\n",
      "</think>\n",
      "\n",
      "Thought: Preciso entender a estrutura do dataset para identificar as colunas relevantes, como 'marca' e 'unidades_vendidas'. \n",
      "Action: buscar_documentos\n",
      "Action Input: \"Quais são as colunas do dataset df_info_dataset?\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m1) REGRAS DE SINTAXE \n",
      " **NÃO INCLUA ASPAS TRIPLAS (```) OU QUALQUER OUTRO DELIMITADOR DE CÓDIGO NO Action Input para 'query_tool'.**\n",
      " **EXEMPLO CERTO:** Action Input: SELECT MARCA, SUM(CPV_UND) FROM df_info_dataset GROUP BY MARCA\n",
      " **EXEMPLO ERRADO:** Action Input: ```SELECT MARCA, SUM(CPV_UND) FROM df_info_dataset GROUP BY MARCA```\n",
      "                  \n",
      " **Regras Específicas para Filtros de Data:**\n",
      " * Ao filtrar por datas, **SEMPRE** utilize o formato `WHERE DATA >= 'YYYY-MM-DD' AND DATA <= 'YYYY-MM-DD'`.\n",
      " * **NUNCA** utilize funções como `year(DATA) = YYYY` ou `month(DATA) = MM` ou qualquer outro método que não seja a comparação direta de datas no formato 'YYYY-MM-DD'.\n",
      " * **EXEMPLO CERTO DE FILTRO DE DATA:** `WHERE DATA >='2025-05-01' AND DATA <= '2024-05-31'` \n",
      "\n",
      " * **EXEMPLO ERRADO DE FILTRO DE DATA:** `WHERE year(DATA) = 2024 AND month(DATA) = 5`\n",
      "\n",
      "**EXEMPLO_3 CONVERSÃO, o usuário pode pedir a conversão de algum produto, esta regra consiste na soma de CPV_UND dividido pela soma de MDTR_UND, deve-se seguir o exemplo da estrutura abaixo e alterando as variáveis conforme necessidade:\n",
      "select sum(CPV_UND) / sum(MDTR_UND)\n",
      "from df_info_dataset\n",
      "group by Marca\n",
      "\n",
      "**EXEMPLO_4 TOTAL DE MARCAS, o usuário pode pedir o total de marcas distintas de algum produto, esta regra consiste na contagem distinta das marcas , deve-se seguir o exemplo da estrutura abaixo e alterando as variáveis conforme necessidade:\n",
      "select count(distinct(Marca))\n",
      "from df_info_dataset\n",
      "group by Marca\n",
      "\n",
      "\n",
      "3) ESTRUTURA DO DATASET, TERMOS & SINÔNIMOS do dataset 'df_info_dataset':\n",
      "* 'DATA': Tipo data (yyyy-mm-dd).\n",
      "* 'MARCA': Tipo texto (ex: 'LUTEAL (A4H)').\n",
      "* 'APRESENTACAO': Tipo texto (ex: 'LUTEAL CAP MOLE BLX60').\n",
      "* 'EAN': Tipo texto/numérico (código, ex: '7896658040894').\n",
      "* 'MDTR_UND': Tipo numérico (Unidades de MDTR).\n",
      "* 'CPV_UND': Tipo numérico (Unidades de CPV).\n",
      "\n",
      "* **EXEMPLO ERRADO DE FILTRO DE DATA:** `WHERE year(DATA) = 2024 AND month(DATA) = 5`\n",
      "\n",
      "\n",
      "2) REGRAS DE NEGOCIO\n",
      "**EXEMPLO_1 SOMA, o usuário pode pedir o valor total de uma marca então deve-se utilizar a ferramenta adequada para query, seguindo o exemplo da estrutura abaixo e alterando as variáveis conforme necessidade:\n",
      "select sum(CPV_UND) \n",
      "from df_info_dataset\n",
      "where Marca = 'MOTORE (A4H)'\n",
      "\n",
      "**EXEMPLO_2 MÉDIA, o usuário pode pedir a média de  períodos então deve-se utilizar a ferramenta adequada para query, seguindo o exemplo da estrutura abaixo e alterando as variáveis conforme necessidade:\n",
      "select avg(CPV_UND) \n",
      "from df_info_dataset\n",
      "where Data >= '2024-01-01' and Data <= '2024-12-31' and Marca = 'ALENIA (A4H)'\n",
      "\n",
      "**Sinônimos e Definições Adicionais:**\n",
      "* Marca pode ser também 'Produto'.\n",
      "* Data pode ser também 'Período, Data ou Ocorrência'.\n",
      "* Apresentacao pode ser também 'SKU'.\n",
      "* EAN pode ser também 'Código'.\n",
      "* CPV_UND pode ser também 'UNIDADES CPV','CPV_UNIDADES','UND_CAMPANHA','UNIDADES CAMPANHA','UNIDADES VENDIDAS'.\n",
      "* MDTR_UND pode ser também 'UNIDADES MDTR', 'MDTR_UNIDADES', 'MDTR'.\n",
      "\n",
      "* Caso o usuário já traga o nome da Marca, não se esqueça de adicionar o sulfixo ' (A4H)' caso ele não tenha escrito.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m<think>\n",
      "</think>\n",
      "\n",
      "Agora que sei as colunas do dataset, posso escrever uma consulta SQL para calcular o total de unidades CPV por marca e identificar a marca com o maior valor.\n",
      "Action: sql_documentos\n",
      "Action Input: SELECT MARCA, SUM(CPV_UND) AS TOTAL_CPV FROM df_info_dataset GROUP BY MARCA ORDER BY TOTAL_CPV DESC LIMIT 1\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m          MARCA  TOTAL_CPV\n",
      "ARTROLIVE (A4H)     541074\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m<think>\n",
      "</think>\n",
      "\n",
      "Final Answer: A marca que mais vendeu, considerando o total de unidades CPV, foi a ARTROLIVE (A4H) com um total de 541.074 unidades vendidas.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Resposta: A marca que mais vendeu, considerando o total de unidades CPV, foi a ARTROLIVE (A4H) com um total de 541.074 unidades vendidas.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Teste 2: Pergunta de cálculo ---\")\n",
    "user_input_2 = \"Considerando o total de unidades CPV, qual foi a marca que mais vendeu?\"\n",
    "response_2 = agent_executor.invoke({\"input\": user_input_2, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=user_input_2), AIMessage(content=response_2[\"output\"])])\n",
    "print(f\"Resposta: {response_2['output']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "074e4bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Teste 3: Pergunta sobre documentos ---\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<think>\n",
      "Okay, let's tackle this question: \"Qual a terceira maior marca vendido em maio de 2024?\" which translates to \"What is the third largest brand sold in May 2024?\" \n",
      "\n",
      "First, I need to understand the data structure and the business rules. The user is asking about sales data, specifically for the month of May 2024, and wants the third highest-selling brand. The dataset is called 'df_info_dataset', so I should start by using the 'buscar_documentos' tool to get an overview of the data and the relevant fields.\n",
      "\n",
      "Using 'buscar_documentos' with a query in natural language to understand the dataset. Maybe start with a general query to see what columns are present, like brand, sales, date, etc. But since the user is asking about May 2024, I should focus on that period. However, the first step is to understand the data structure. Let me try that.\n",
      "\n",
      "Action: buscar_documentos\n",
      "Action Input: \"Mostar as colunas e as regras de negócio do dataset df_info_dataset\"\n",
      "\n",
      "Wait, maybe the user's initial query is in Portuguese, so I should use Portuguese for the action input. Let me adjust that.\n",
      "\n",
      "Action: buscar_documentos\n",
      "Action Input: \"Mostre as colunas e as regras de negócio do dataset df_info_dataset\"\n",
      "\n",
      "Assuming the response from the tool gives information about the columns, like 'marca' (brand), 'valor_venda' (sales value), 'data_venda' (sale date), etc. Once I know the structure, I can proceed to write an SQL query to find the third largest brand in May 2024.\n",
      "\n",
      "But first, I need to confirm the date format. If the 'data_venda' is in a date format, I can filter for May 2024. Then, aggregate the sales per brand, sort them in descending order, and pick the third one. However, the user might be referring to the total sales volume or quantity. Since the question mentions \"vendido\" (sold), it's possible they mean total sales value. But I should check if the dataset has a 'quantidade' (quantity) field or 'valor_venda' (sales value). \n",
      "\n",
      "Assuming the dataset has 'marca', 'valor_venda', and 'data_venda'. The steps would be:\n",
      "\n",
      "1. Filter all sales records where the sale date is in May 2024.\n",
      "2. Group by brand and sum the sales value for each brand.\n",
      "3. Order the brands in descending order of total sales.\n",
      "4. Select the third brand in this ordered list.\n",
      "\n",
      "So, the SQL query would look something like:\n",
      "\n",
      "SELECT marca, SUM(valor_venda) AS total_vendas\n",
      "FROM df_info_dataset\n",
      "WHERE data_venda BETWEEN '2024-05-01' AND '2024-05-31'\n",
      "GROUP BY marca\n",
      "ORDER BY total_vendas DESC\n",
      "LIMIT 1 OFFSET 2;\n",
      "\n",
      "But wait, the user might have different date formatting. Also, need to make sure that the 'data_venda' is in a date type. If the tool's response confirms that, then the query is correct. \n",
      "\n",
      "Alternatively, if the dataset uses a different date format, like 'YYYY-MM-DD', the BETWEEN clause would work. Also, if there are multiple entries per brand, grouping by marca and summing the sales makes sense.\n",
      "\n",
      "Once the SQL query is executed, the result should give the third brand. However, I need to first use 'buscar_documentos' to confirm the data structure and the correct columns to use. Let me proceed with that first step.\n",
      "</think>\n",
      "\n",
      "Thought: Preciso entender a estrutura do dataset para formular a consulta correta. Primeiro, uso a ferramenta buscar_documentos para obter informações sobre as colunas e regras de negócio.\n",
      "\n",
      "Action: buscar_documentos\n",
      "Action Input: \"Mostre as colunas e as regras de negócio do dataset df_info_dataset\"\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m1) REGRAS DE SINTAXE \n",
      " **NÃO INCLUA ASPAS TRIPLAS (```) OU QUALQUER OUTRO DELIMITADOR DE CÓDIGO NO Action Input para 'query_tool'.**\n",
      " **EXEMPLO CERTO:** Action Input: SELECT MARCA, SUM(CPV_UND) FROM df_info_dataset GROUP BY MARCA\n",
      " **EXEMPLO ERRADO:** Action Input: ```SELECT MARCA, SUM(CPV_UND) FROM df_info_dataset GROUP BY MARCA```\n",
      "                  \n",
      " **Regras Específicas para Filtros de Data:**\n",
      " * Ao filtrar por datas, **SEMPRE** utilize o formato `WHERE DATA >= 'YYYY-MM-DD' AND DATA <= 'YYYY-MM-DD'`.\n",
      " * **NUNCA** utilize funções como `year(DATA) = YYYY` ou `month(DATA) = MM` ou qualquer outro método que não seja a comparação direta de datas no formato 'YYYY-MM-DD'.\n",
      " * **EXEMPLO CERTO DE FILTRO DE DATA:** `WHERE DATA >='2025-05-01' AND DATA <= '2024-05-31'` \n",
      "\n",
      " * **EXEMPLO ERRADO DE FILTRO DE DATA:** `WHERE year(DATA) = 2024 AND month(DATA) = 5`\n",
      "\n",
      "**EXEMPLO_3 CONVERSÃO, o usuário pode pedir a conversão de algum produto, esta regra consiste na soma de CPV_UND dividido pela soma de MDTR_UND, deve-se seguir o exemplo da estrutura abaixo e alterando as variáveis conforme necessidade:\n",
      "select sum(CPV_UND) / sum(MDTR_UND)\n",
      "from df_info_dataset\n",
      "group by Marca\n",
      "\n",
      "**EXEMPLO_4 TOTAL DE MARCAS, o usuário pode pedir o total de marcas distintas de algum produto, esta regra consiste na contagem distinta das marcas , deve-se seguir o exemplo da estrutura abaixo e alterando as variáveis conforme necessidade:\n",
      "select count(distinct(Marca))\n",
      "from df_info_dataset\n",
      "group by Marca\n",
      "\n",
      "\n",
      "3) ESTRUTURA DO DATASET, TERMOS & SINÔNIMOS do dataset 'df_info_dataset':\n",
      "* 'DATA': Tipo data (yyyy-mm-dd).\n",
      "* 'MARCA': Tipo texto (ex: 'LUTEAL (A4H)').\n",
      "* 'APRESENTACAO': Tipo texto (ex: 'LUTEAL CAP MOLE BLX60').\n",
      "* 'EAN': Tipo texto/numérico (código, ex: '7896658040894').\n",
      "* 'MDTR_UND': Tipo numérico (Unidades de MDTR).\n",
      "* 'CPV_UND': Tipo numérico (Unidades de CPV).\n",
      "\n",
      "* **EXEMPLO ERRADO DE FILTRO DE DATA:** `WHERE year(DATA) = 2024 AND month(DATA) = 5`\n",
      "\n",
      "\n",
      "2) REGRAS DE NEGOCIO\n",
      "**EXEMPLO_1 SOMA, o usuário pode pedir o valor total de uma marca então deve-se utilizar a ferramenta adequada para query, seguindo o exemplo da estrutura abaixo e alterando as variáveis conforme necessidade:\n",
      "select sum(CPV_UND) \n",
      "from df_info_dataset\n",
      "where Marca = 'MOTORE (A4H)'\n",
      "\n",
      "**EXEMPLO_2 MÉDIA, o usuário pode pedir a média de  períodos então deve-se utilizar a ferramenta adequada para query, seguindo o exemplo da estrutura abaixo e alterando as variáveis conforme necessidade:\n",
      "select avg(CPV_UND) \n",
      "from df_info_dataset\n",
      "where Data >= '2024-01-01' and Data <= '2024-12-31' and Marca = 'ALENIA (A4H)'\n",
      "\n",
      "**Sinônimos e Definições Adicionais:**\n",
      "* Marca pode ser também 'Produto'.\n",
      "* Data pode ser também 'Período, Data ou Ocorrência'.\n",
      "* Apresentacao pode ser também 'SKU'.\n",
      "* EAN pode ser também 'Código'.\n",
      "* CPV_UND pode ser também 'UNIDADES CPV','CPV_UNIDADES','UND_CAMPANHA','UNIDADES CAMPANHA','UNIDADES VENDIDAS'.\n",
      "* MDTR_UND pode ser também 'UNIDADES MDTR', 'MDTR_UNIDADES', 'MDTR'.\n",
      "\n",
      "* Caso o usuário já traga o nome da Marca, não se esqueça de adicionar o sulfixo ' (A4H)' caso ele não tenha escrito.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m<think>\n",
      "</think>\n",
      "\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3m<think>\n",
      "</think>\n",
      "\n",
      "Thought: Agora que sei a estrutura do dataset, posso formular a consulta SQL para encontrar a terceira maior marca vendida em maio de 2024. \n",
      "\n",
      "Action: sql_documentos\n",
      "Action Input: \n",
      "SELECT marca, SUM(CPV_UND) AS total_vendas\n",
      "FROM df_info_dataset\n",
      "WHERE DATA >= '2024-05-01' AND DATA <= '2024-05-31'\n",
      "GROUP BY marca\n",
      "ORDER BY total_vendas DESC\n",
      "LIMIT 1 OFFSET 2;\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m       MARCA  total_vendas\n",
      "MOTORE (A4H)         14655\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m<think>\n",
      "</think>\n",
      "\n",
      "Final Answer: A terceira maior marca vendida em maio de 2024 foi MOTORE (A4H).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Resposta: A terceira maior marca vendida em maio de 2024 foi MOTORE (A4H).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Teste 3: Pergunta sobre documentos ---\")\n",
    "user_input_3 = \"Qual a terceira maior marca vendido em maio de 2024?\"\n",
    "response_3 = agent_executor.invoke({\"input\": user_input_3, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=user_input_3), AIMessage(content=response_3[\"output\"])])\n",
    "print(f\"Resposta: {response_3['output']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "888409b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Teste 4: Outra pergunta sobre documentos ---\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<think>\n",
      "Okay, the user is asking for the conversion of \"Marca Motore\" in June 2024. First, I need to understand what exactly they mean by \"conversão.\" In business terms, conversion could refer to sales volume, revenue, conversion rate (like website visitors to customers), or something else specific to their operations. Since the dataset is called 'df_info_dataset', I should start by using the 'buscar_documentos' tool to get more context about the data and what \"Marca Motore\" entails.\n",
      "\n",
      "I'll input the query to the 'buscar_documentos' tool to see if there's any existing information or definitions related to \"Marca Motore\" and conversions. Maybe the dataset has specific metrics or tables that define these terms. Let me check that first.\n",
      "</think>\n",
      "\n",
      "Thought: Preciso entender o contexto da pergunta e o que se refere à \"conversão da Marca Motore\". Vou usar a ferramenta buscar_documentos para obter informações sobre o dataset e a definição de \"conversão\" no contexto da marca.\n",
      "\n",
      "Action: buscar_documentos\n",
      "Action Input: \"O que significa 'conversão da Marca Motore' no dataset df_info_dataset?\"\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m**EXEMPLO_3 CONVERSÃO, o usuário pode pedir a conversão de algum produto, esta regra consiste na soma de CPV_UND dividido pela soma de MDTR_UND, deve-se seguir o exemplo da estrutura abaixo e alterando as variáveis conforme necessidade:\n",
      "select sum(CPV_UND) / sum(MDTR_UND)\n",
      "from df_info_dataset\n",
      "group by Marca\n",
      "\n",
      "**EXEMPLO_4 TOTAL DE MARCAS, o usuário pode pedir o total de marcas distintas de algum produto, esta regra consiste na contagem distinta das marcas , deve-se seguir o exemplo da estrutura abaixo e alterando as variáveis conforme necessidade:\n",
      "select count(distinct(Marca))\n",
      "from df_info_dataset\n",
      "group by Marca\n",
      "\n",
      "\n",
      "3) ESTRUTURA DO DATASET, TERMOS & SINÔNIMOS do dataset 'df_info_dataset':\n",
      "* 'DATA': Tipo data (yyyy-mm-dd).\n",
      "* 'MARCA': Tipo texto (ex: 'LUTEAL (A4H)').\n",
      "* 'APRESENTACAO': Tipo texto (ex: 'LUTEAL CAP MOLE BLX60').\n",
      "* 'EAN': Tipo texto/numérico (código, ex: '7896658040894').\n",
      "* 'MDTR_UND': Tipo numérico (Unidades de MDTR).\n",
      "* 'CPV_UND': Tipo numérico (Unidades de CPV).\n",
      "\n",
      "**Sinônimos e Definições Adicionais:**\n",
      "* Marca pode ser também 'Produto'.\n",
      "* Data pode ser também 'Período, Data ou Ocorrência'.\n",
      "* Apresentacao pode ser também 'SKU'.\n",
      "* EAN pode ser também 'Código'.\n",
      "* CPV_UND pode ser também 'UNIDADES CPV','CPV_UNIDADES','UND_CAMPANHA','UNIDADES CAMPANHA','UNIDADES VENDIDAS'.\n",
      "* MDTR_UND pode ser também 'UNIDADES MDTR', 'MDTR_UNIDADES', 'MDTR'.\n",
      "\n",
      "* Caso o usuário já traga o nome da Marca, não se esqueça de adicionar o sulfixo ' (A4H)' caso ele não tenha escrito.\n",
      "\n",
      "1) REGRAS DE SINTAXE \n",
      " **NÃO INCLUA ASPAS TRIPLAS (```) OU QUALQUER OUTRO DELIMITADOR DE CÓDIGO NO Action Input para 'query_tool'.**\n",
      " **EXEMPLO CERTO:** Action Input: SELECT MARCA, SUM(CPV_UND) FROM df_info_dataset GROUP BY MARCA\n",
      " **EXEMPLO ERRADO:** Action Input: ```SELECT MARCA, SUM(CPV_UND) FROM df_info_dataset GROUP BY MARCA```\n",
      "                  \n",
      " **Regras Específicas para Filtros de Data:**\n",
      " * Ao filtrar por datas, **SEMPRE** utilize o formato `WHERE DATA >= 'YYYY-MM-DD' AND DATA <= 'YYYY-MM-DD'`.\n",
      " * **NUNCA** utilize funções como `year(DATA) = YYYY` ou `month(DATA) = MM` ou qualquer outro método que não seja a comparação direta de datas no formato 'YYYY-MM-DD'.\n",
      " * **EXEMPLO CERTO DE FILTRO DE DATA:** `WHERE DATA >='2025-05-01' AND DATA <= '2024-05-31'` \n",
      "\n",
      " * **EXEMPLO ERRADO DE FILTRO DE DATA:** `WHERE year(DATA) = 2024 AND month(DATA) = 5`\n",
      "\n",
      "* **EXEMPLO ERRADO DE FILTRO DE DATA:** `WHERE year(DATA) = 2024 AND month(DATA) = 5`\n",
      "\n",
      "\n",
      "2) REGRAS DE NEGOCIO\n",
      "**EXEMPLO_1 SOMA, o usuário pode pedir o valor total de uma marca então deve-se utilizar a ferramenta adequada para query, seguindo o exemplo da estrutura abaixo e alterando as variáveis conforme necessidade:\n",
      "select sum(CPV_UND) \n",
      "from df_info_dataset\n",
      "where Marca = 'MOTORE (A4H)'\n",
      "\n",
      "**EXEMPLO_2 MÉDIA, o usuário pode pedir a média de  períodos então deve-se utilizar a ferramenta adequada para query, seguindo o exemplo da estrutura abaixo e alterando as variáveis conforme necessidade:\n",
      "select avg(CPV_UND) \n",
      "from df_info_dataset\n",
      "where Data >= '2024-01-01' and Data <= '2024-12-31' and Marca = 'ALENIA (A4H)'\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m<think>\n",
      "</think>\n",
      "\n",
      "A \"conversão da Marca Motore\" no contexto do dataset refere-se à razão entre a soma de \"CPV_UND\" e a soma de \"MDTR_UND\" para a marca \"Motore (A4H)\". Para calcular isso em junho de 2024, precisamos filtrar os registros para o mês de junho de 2024 e aplicar a fórmula de conversão.\n",
      "\n",
      "Action: sql_documentos\n",
      "Action Input: \n",
      "select \n",
      "  sum(CPV_UND) / sum(MDTR_UND) as conversao\n",
      "from df_info_dataset\n",
      "where Marca = 'MOTORE (A4H)'\n",
      "and DATA >= '2024-06-01'\n",
      "and DATA <= '2024-06-30'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m0.7197845636920644\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m<think>\n",
      "</think>\n",
      "\n",
      "Final Answer: A conversão da Marca Motore em junho de 2024 foi de aproximadamente 0,72 (ou 72%).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Resposta: A conversão da Marca Motore em junho de 2024 foi de aproximadamente 0,72 (ou 72%).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Teste 4: Outra pergunta sobre documentos ---\")\n",
    "user_input_4 = \"Me diga a conversão da Marca Motore em junho de 2024\"\n",
    "response_4 = agent_executor.invoke({\"input\": user_input_4, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=user_input_4), AIMessage(content=response_4[\"output\"])])\n",
    "print(f\"Resposta: {response_4['output']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
