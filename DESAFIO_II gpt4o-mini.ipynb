{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "j7Kb3ABZkv_M",
        "outputId": "9bf3ec6c-d8a4-49da-a630-dbd5557f0575"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-community in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.3.24)\n",
            "Requirement already satisfied: openai in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.84.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.3.0)\n",
            "Requirement already satisfied: pandasql in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.7.3)\n",
            "Requirement already satisfied: python-dotenv in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.3.63)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.3.43)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (3.12.7)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (2.2.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>4->openai) (0.4.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\pichau\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
            "[notice] To update, run: C:\\Users\\Pichau\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#0) instalar todas as dependencias\n",
        "%pip install langchain langchain-community openai pandas pandasql python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#1) Importaçao das bibliotecas necessárias\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.tools import Tool\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from pandasql import sqldf\n",
        "from io import TextIOWrapper\n",
        "import re \n",
        "\n",
        "#import pandasql as ps\n",
        "#from openai import OpenAI\n",
        "#from langchain_core.tools import Tool\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Variável global para armazenar o dataframe\n",
        "df_global = None\n",
        "\n",
        "#2) Criação da função das ferramentas\n",
        "def carregar_arquivos(arquivo_zip_path: str):\n",
        "    \"\"\"\n",
        "    **PRIMEIRA ETAPA SEMPRE**\n",
        "    Quando necessário extrai os arquivos do formato .zip (Extrair,Extract,Descompactar,Unzip,Abrir,Open,Deszipar,Decompress,Desempacotar,Unarchive,Desarquivar)\n",
        "    Exemplo: 'C:\\\\Users\\Pichau\\Downloads\\\\teste\\\\202401_NFs.zip'\n",
        "    Exemplo2: 'C:\\\\Users\\\\ABC\\\\Documents\\\\arquivo.zip'\n",
        "    Exemplo3: 'C:\\\\_______NOME.zip'\n",
        "    \"\"\"\n",
        "    arquivo_zip_path = arquivo_zip_path.strip(\"'\\\"\")\n",
        "    global df_global # Declara que você vai modificar a variável global\n",
        "    grupo_csv = {}\n",
        "    \n",
        "    with zipfile.ZipFile(arquivo_zip_path, 'r') as zip_ref:\n",
        "        for arquivos in zip_ref.namelist():\n",
        "            if arquivos.endswith('.csv'):\n",
        "                with zip_ref.open(arquivos) as arq:\n",
        "                    # Adicionado errors='ignore' para tentar lidar com problemas de codificação\n",
        "                    df = pd.read_csv(TextIOWrapper(arq,'utf-8', errors='ignore'))\n",
        "                    grupo_csv[arquivos] = df\n",
        "            else:\n",
        "                print(f\"Sem suporte para o arquivo: '{arquivos}' não é um arquivo .csv\")\n",
        "\n",
        "\n",
        "        sorted_keys = sorted(grupo_csv.keys())\n",
        "        primeiro_df = grupo_csv[sorted_keys[0]]\n",
        "        segundo_df = grupo_csv[sorted_keys[1]]\n",
        "       \n",
        "        df_global = pd.merge(primeiro_df, segundo_df, on='CHAVE DE ACESSO', how='left')\n",
        "        df_global.columns = df_global.columns.str.replace('_x$', '', regex=True)\n",
        "        df_global = df_global.drop(columns=['MODELO_y','SÉRIE_y','NÚMERO_y','NATUREZA DA OPERAÇÃO_y','DATA EMISSÃO_y','CPF/CNPJ Emitente_y','RAZÃO SOCIAL EMITENTE_y','INSCRIÇÃO ESTADUAL EMITENTE_y','UF EMITENTE_y','MUNICÍPIO EMITENTE_y','CNPJ DESTINATÁRIO_y','NOME DESTINATÁRIO_y','UF DESTINATÁRIO_y','INDICADOR IE DESTINATÁRIO_y','DESTINO DA OPERAÇÃO_y','CONSUMIDOR FINAL_y','PRESENÇA DO COMPRADOR_y'])\n",
        "\n",
        "\n",
        "        return f\"Arquivos carregados e merged com sucesso! DataFrame pronto para análise com {len(df_global)} linhas e {len(df_global.columns)} colunas. As colunas disponíveis são: {list(df_global.columns)}.\"\n",
        "\n",
        "def consulta_sql(query: str) -> str:\n",
        "    \"\"\"\n",
        "    **SEGUNDA ETAPA SEMPRE**\n",
        "    Executa uma consulta SQL no DataFrame df_global.\n",
        "    \"\"\"\n",
        "    global df_global\n",
        "    if df_global is None:\n",
        "        return \"O DataFrame global ainda não foi carregado.\"\n",
        "\n",
        "    try:\n",
        "        result = sqldf(query, {'df_global': df_global})\n",
        "        return result.to_string(index=False)\n",
        "    except Exception as e:\n",
        "        return f\"Erro na consulta: {e}\"\n",
        "    \n",
        "\n",
        "def mapear_colunas(args: dict) -> str:\n",
        "    \"\"\"\n",
        "    **TERCEIRA ETAPA OPCIONAL**\n",
        "    Caso tenha dificuldades em encontrar o nome da coluna adequadamente, utilize essa ferramenta que mapeia os nomes das colunas do DataFrame global (df_global) para nomes mais simples e padronizados.\n",
        "    Essa ferramenta deve ser usada quando a consulta SQL falhar por nomes de colunas incorretos ou complexos.\n",
        "    \"\"\"\n",
        "    import re\n",
        "    \n",
        "    global df_global\n",
        "    if df_global is None:\n",
        "        return \"O DataFrame global (df_global) ainda não foi carregado.\"\n",
        "\n",
        "    colunas_originais = df_global.columns.tolist()\n",
        "    \n",
        "    # Cria dicionário de mapeamento simples\n",
        "    mapeamento = {}\n",
        "    for col in colunas_originais:\n",
        "        col_nova = col.upper()\n",
        "        col_nova = re.sub(r\"[^A-Z0-9]\", \"_\", col_nova)  # Substitui espaços/símbolos por _\n",
        "        col_nova = re.sub(r\"_+\", \"_\", col_nova).strip(\"_\")  # Remove múltiplos _\n",
        "        mapeamento[col] = col_nova\n",
        "\n",
        "    # Aplica o mapeamento\n",
        "    df_global.columns = [mapeamento[col] for col in df_global.columns]\n",
        "    \n",
        "    retorno = \"\\n\".join([f\"{orig} -> {novo}\" for orig, novo in mapeamento.items()])\n",
        "    return f\"Colunas renomeadas com sucesso:\\n{retorno}\"    \n",
        "\n",
        "#3) Criando a ferramenta\n",
        "zip_tool = Tool(\n",
        "    name=\"carregar_arquivos\",\n",
        "    func=carregar_arquivos,\n",
        "    description=carregar_arquivos.__doc__.strip() # Pega a docstring da função como descrição,\n",
        ")\n",
        "\n",
        "query_tool = Tool(\n",
        "name=\"query_arquivo\",\n",
        "    func=consulta_sql,\n",
        "    description=consulta_sql.__doc__.strip() # Pega a docstring da função como descrição,\n",
        ")\n",
        "\n",
        "map_tool = Tool(\n",
        "name='mapear_colunas',\n",
        "func=mapear_colunas,\n",
        "description=mapear_colunas.__doc__.strip() # Pega a docstring da função como descrição\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Pichau\\AppData\\Local\\Temp\\ipykernel_6368\\717820186.py:4: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(\n",
            "C:\\Users\\Pichau\\AppData\\Local\\Temp\\ipykernel_6368\\717820186.py:14: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = initialize_agent(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "load_dotenv() #carrega o segredo\n",
        "\n",
        "# 4) Escolhe o modelo selecionado e configuração de temperatura\n",
        "llm = ChatOpenAI(\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    temperature=0.25,\n",
        "    api_key = os.getenv(\"api_key\"),\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "   \n",
        "# 5) Inicializando o agente com a ferramenta\n",
        "agent = initialize_agent(\n",
        "    tools=[zip_tool,query_tool,map_tool],\n",
        "    llm=llm,\n",
        "    agent_type = AgentType.OPENAI_MULTI_FUNCTIONS,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Pichau\\AppData\\Local\\Temp\\ipykernel_6368\\4289259528.py:4: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  resposta = agent.run(user_input)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to start by extracting the contents of the ZIP file located at 'C:\\Users\\Pichau\\Downloads\\teste\\202401_NFs.zip'. This will allow me to access the data inside it for further analysis.\n",
            "\n",
            "Action: carregar_arquivos  \n",
            "Action Input: 'C:\\Users\\Pichau\\Downloads\\teste\\202401_NFs.zip'  \u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mArquivos carregados e merged com sucesso! DataFrame pronto para análise com 565 linhas e 30 colunas. As colunas disponíveis são: ['CHAVE DE ACESSO', 'MODELO', 'SÉRIE', 'NÚMERO', 'NATUREZA DA OPERAÇÃO', 'DATA EMISSÃO', 'EVENTO MAIS RECENTE', 'DATA/HORA EVENTO MAIS RECENTE', 'CPF/CNPJ Emitente', 'RAZÃO SOCIAL EMITENTE', 'INSCRIÇÃO ESTADUAL EMITENTE', 'UF EMITENTE', 'MUNICÍPIO EMITENTE', 'CNPJ DESTINATÁRIO', 'NOME DESTINATÁRIO', 'UF DESTINATÁRIO', 'INDICADOR IE DESTINATÁRIO', 'DESTINO DA OPERAÇÃO', 'CONSUMIDOR FINAL', 'PRESENÇA DO COMPRADOR', 'VALOR NOTA FISCAL', 'NÚMERO PRODUTO', 'DESCRIÇÃO DO PRODUTO/SERVIÇO', 'CÓDIGO NCM/SH', 'NCM/SH (TIPO DE PRODUTO)', 'CFOP', 'QUANTIDADE', 'UNIDADE', 'VALOR UNITÁRIO', 'VALOR TOTAL'].\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mAgora que extraí os arquivos e tenho acesso ao DataFrame, posso realizar uma consulta para encontrar o menor valor da coluna 'VALOR TOTAL'.\n",
            "\n",
            "Action: query_arquivo  \n",
            "Action Input: \"SELECT MIN(`VALOR TOTAL`) AS menor_valor FROM df_global\"  \u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m menor_valor\n",
            "        2.18\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
            "Final Answer: O menor valor da coluna 'Valor total' é 2.18.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Resposta: O menor valor da coluna 'Valor total' é 2.18.\n"
          ]
        }
      ],
      "source": [
        "#6) Recebe a pergunta do usuário\n",
        "user_input = \"Extraia o arquivo localizado em: 'C:\\\\Users\\\\Pichau\\\\Downloads\\\\teste\\\\202401_NFs.zip' Qual o menor valor da coluna 'Valor total'?\"\n",
        "# Executa o agente passando a pergunta\n",
        "resposta = agent.run(user_input)\n",
        "\n",
        "print(f\"Resposta: {resposta}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to extract the contents of the specified ZIP file before I can analyze the data within it. Therefore, I will use the `carregar_arquivos` action to extract the files.\n",
            "\n",
            "Action: carregar_arquivos  \n",
            "Action Input: 'C:\\Users\\Pichau\\Downloads\\teste\\202401_NFs.zip'  \u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mArquivos carregados e merged com sucesso! DataFrame pronto para análise com 565 linhas e 30 colunas. As colunas disponíveis são: ['CHAVE DE ACESSO', 'MODELO', 'SÉRIE', 'NÚMERO', 'NATUREZA DA OPERAÇÃO', 'DATA EMISSÃO', 'EVENTO MAIS RECENTE', 'DATA/HORA EVENTO MAIS RECENTE', 'CPF/CNPJ Emitente', 'RAZÃO SOCIAL EMITENTE', 'INSCRIÇÃO ESTADUAL EMITENTE', 'UF EMITENTE', 'MUNICÍPIO EMITENTE', 'CNPJ DESTINATÁRIO', 'NOME DESTINATÁRIO', 'UF DESTINATÁRIO', 'INDICADOR IE DESTINATÁRIO', 'DESTINO DA OPERAÇÃO', 'CONSUMIDOR FINAL', 'PRESENÇA DO COMPRADOR', 'VALOR NOTA FISCAL', 'NÚMERO PRODUTO', 'DESCRIÇÃO DO PRODUTO/SERVIÇO', 'CÓDIGO NCM/SH', 'NCM/SH (TIPO DE PRODUTO)', 'CFOP', 'QUANTIDADE', 'UNIDADE', 'VALOR UNITÁRIO', 'VALOR TOTAL'].\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI need to find the maximum value in the 'VALOR TOTAL' column of the DataFrame. I will use the `query_arquivo` action to perform this operation.\n",
            "\n",
            "Action: query_arquivo  \n",
            "Action Input: \"SELECT MAX(`VALOR TOTAL`) AS max_valor_total FROM df_global\"  \u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m max_valor_total\n",
            "        985050.0\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
            "Final Answer: O maior valor da coluna 'Valor total' é 985050.0.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Resposta: O maior valor da coluna 'Valor total' é 985050.0.\n"
          ]
        }
      ],
      "source": [
        "#6.1) Recebe a pergunta do usuário\n",
        "user_input1 = \"Extraia o arquivo localizado em: 'C:\\\\Users\\\\Pichau\\\\Downloads\\\\teste\\\\202401_NFs.zip' Qual o maior valor da coluna 'Valor total'?\"\n",
        "# Executa o agente passando a pergunta\n",
        "resposta = agent.run(user_input1)\n",
        "\n",
        "print(f\"Resposta: {resposta}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to extract the contents of the specified ZIP file first to access the data inside it. \n",
            "\n",
            "Action: carregar_arquivos  \n",
            "Action Input: 'C:\\Users\\Pichau\\Downloads\\teste\\202401_NFs.zip'  \u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mArquivos carregados e merged com sucesso! DataFrame pronto para análise com 565 linhas e 30 colunas. As colunas disponíveis são: ['CHAVE DE ACESSO', 'MODELO', 'SÉRIE', 'NÚMERO', 'NATUREZA DA OPERAÇÃO', 'DATA EMISSÃO', 'EVENTO MAIS RECENTE', 'DATA/HORA EVENTO MAIS RECENTE', 'CPF/CNPJ Emitente', 'RAZÃO SOCIAL EMITENTE', 'INSCRIÇÃO ESTADUAL EMITENTE', 'UF EMITENTE', 'MUNICÍPIO EMITENTE', 'CNPJ DESTINATÁRIO', 'NOME DESTINATÁRIO', 'UF DESTINATÁRIO', 'INDICADOR IE DESTINATÁRIO', 'DESTINO DA OPERAÇÃO', 'CONSUMIDOR FINAL', 'PRESENÇA DO COMPRADOR', 'VALOR NOTA FISCAL', 'NÚMERO PRODUTO', 'DESCRIÇÃO DO PRODUTO/SERVIÇO', 'CÓDIGO NCM/SH', 'NCM/SH (TIPO DE PRODUTO)', 'CFOP', 'QUANTIDADE', 'UNIDADE', 'VALOR UNITÁRIO', 'VALOR TOTAL'].\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI need to find the second highest value in the 'VALOR TOTAL' column and then retrieve the corresponding 'RAZÃO SOCIAL EMITENTE'. \n",
            "\n",
            "Action: query_arquivo  \n",
            "Action Input: \"SELECT [RAZÃO SOCIAL EMITENTE], [VALOR TOTAL] FROM df_global ORDER BY [VALOR TOTAL] DESC LIMIT 1 OFFSET 1;\"  \u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m   RAZÃO SOCIAL EMITENTE  VALOR TOTAL\n",
            "LABORATORIOS B.BRAUN S.A     726081.6\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
            "Final Answer: A razão social que teve o segundo maior valor da coluna 'Valor total' é \"LABORATORIOS B.BRAUN S.A\" com um valor de 726081.6.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Resposta: A razão social que teve o segundo maior valor da coluna 'Valor total' é \"LABORATORIOS B.BRAUN S.A\" com um valor de 726081.6.\n"
          ]
        }
      ],
      "source": [
        "#6.2) Recebe a pergunta do usuário\n",
        "user_input1 = \"Extraia o arquivo localizado em: 'C:\\\\Users\\\\Pichau\\\\Downloads\\\\teste\\\\202401_NFs.zip' Qual a 'Razao social' que teve o segundo maior valor da coluna 'Valor total'?\"\n",
        "# Executa o agente passando a pergunta\n",
        "resposta = agent.run(user_input1)\n",
        "\n",
        "print(f\"Resposta: {resposta}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to extract the contents of the specified ZIP file first to access the data within it. \n",
            "\n",
            "Action: carregar_arquivos  \n",
            "Action Input: 'C:\\Users\\Pichau\\Downloads\\teste\\202401_NFs.zip'  \u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mArquivos carregados e merged com sucesso! DataFrame pronto para análise com 565 linhas e 30 colunas. As colunas disponíveis são: ['CHAVE DE ACESSO', 'MODELO', 'SÉRIE', 'NÚMERO', 'NATUREZA DA OPERAÇÃO', 'DATA EMISSÃO', 'EVENTO MAIS RECENTE', 'DATA/HORA EVENTO MAIS RECENTE', 'CPF/CNPJ Emitente', 'RAZÃO SOCIAL EMITENTE', 'INSCRIÇÃO ESTADUAL EMITENTE', 'UF EMITENTE', 'MUNICÍPIO EMITENTE', 'CNPJ DESTINATÁRIO', 'NOME DESTINATÁRIO', 'UF DESTINATÁRIO', 'INDICADOR IE DESTINATÁRIO', 'DESTINO DA OPERAÇÃO', 'CONSUMIDOR FINAL', 'PRESENÇA DO COMPRADOR', 'VALOR NOTA FISCAL', 'NÚMERO PRODUTO', 'DESCRIÇÃO DO PRODUTO/SERVIÇO', 'CÓDIGO NCM/SH', 'NCM/SH (TIPO DE PRODUTO)', 'CFOP', 'QUANTIDADE', 'UNIDADE', 'VALOR UNITÁRIO', 'VALOR TOTAL'].\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mAgora que extraí os arquivos e tenho acesso ao DataFrame, preciso consultar qual 'Tipo de produto' teve a maior 'quantidade'. Vou executar uma consulta SQL para obter essa informação.\n",
            "\n",
            "Action: query_arquivo  \n",
            "Action Input: \"SELECT [NCM/SH (TIPO DE PRODUTO)], SUM(QUANTIDADE) as total_quantidade FROM df_global GROUP BY [NCM/SH (TIPO DE PRODUTO)] ORDER BY total_quantidade DESC LIMIT 1;\"  \u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m                        NCM/SH (TIPO DE PRODUTO)  total_quantidade\n",
            "Outros livros, brochuras e impressos semelhantes           68684.0\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
            "Final Answer: O 'Tipo de produto' que teve a maior 'quantidade' é \"Outros livros, brochuras e impressos semelhantes\" com um total de 68.684 unidades.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Resposta: O 'Tipo de produto' que teve a maior 'quantidade' é \"Outros livros, brochuras e impressos semelhantes\" com um total de 68.684 unidades.\n"
          ]
        }
      ],
      "source": [
        "#6.3) Recebe a pergunta do usuário\n",
        "user_input1 = \"Extraia o arquivo localizado em: 'C:\\\\Users\\\\Pichau\\\\Downloads\\\\teste\\\\202401_NFs.zip' Qual 'Tipo de produto' teve maior 'quantidade'?\"\n",
        "# Executa o agente passando a pergunta\n",
        "resposta = agent.run(user_input1)\n",
        "\n",
        "print(f\"Resposta: {resposta}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
